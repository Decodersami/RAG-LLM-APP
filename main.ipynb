{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: blue; padding: 20px; border-radius: 5px;\">\n",
    "    <h1 style=\"color:white;\">Retrieval Augmented Generation App</h1>\n",
    "    <h2 style=\"color:while\">Using Llamaindex and OpenAI-Indexing and Querying Multiple pdf's</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents=SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='ec44f640-c6ac-46cb-8d58-a17fa6317270', embedding=None, metadata={'page_label': '1', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Active Retrieval Augmented Generation\\nZhengbao Jiang1∗Frank F. Xu1∗Luyu Gao1∗Zhiqing Sun1∗Qian Liu2\\nJane Dwivedi-Yu3Yiming Yang1Jamie Callan1Graham Neubig1\\n1Language Technologies Institute, Carnegie Mellon University\\n2Sea AI Lab3FAIR, Meta\\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\\nAbstract\\nDespite the remarkable ability of large lan-\\nguage models (LMs) to comprehend and gen-\\nerate language, they have a tendency to hal-\\nlucinate and create factually inaccurate out-\\nput. Augmenting LMs by retrieving informa-\\ntion from external knowledge resources is one\\npromising solution. Most existing retrieval aug-\\nmented LMs employ a retrieve-and-generate\\nsetup that only retrieves information once based\\non the input. This is limiting, however, in\\nmore general scenarios involving generation\\nof long texts, where continually gathering in-\\nformation throughout generation is essential. In\\nthis work, we provide a generalized view of ac-\\ntive retrieval augmented generation , methods\\nthat actively decide when and what to retrieve\\nacross the course of the generation. We propose\\nForward- Looking Active REtrieval augmented\\ngeneration ( FLARE ), a generic method which\\niteratively uses a prediction of the upcoming\\nsentence to anticipate future content, which is\\nthen utilized as a query to retrieve relevant doc-\\numents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along\\nwith baselines comprehensively over 4 long-\\nform knowledge-intensive generation tasks/-\\ndatasets. FLARE achieves superior or compet-\\nitive performance on all tasks, demonstrating\\nthe effectiveness of our method.1\\n1 Introduction\\nGenerative language models (LMs) (Brown et al.,\\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-\\nhery et al., 2022; Zhang et al., 2022; Touvron et al.,\\n2023; Zhao et al., 2023) have become a founda-\\ntional component in natural language processing\\n(NLP) systems with their remarkable abilities. Al-\\nthough LMs have memorized some world knowl-\\nedge during training (Petroni et al., 2019; Roberts\\net al., 2020; Jiang et al., 2020), they still tend to\\n∗Lead contributors.\\n1Code and datasets are available at https://github.com/\\njzbjyb/FLARE .hallucinate and create imaginary content (Maynez\\net al., 2020; Zhou et al., 2021). Augmenting LMs\\nwith retrieval components that look up relevant in-\\nformation from external knowledge resources is a\\npromising direction to address hallucination (Khan-\\ndelwal et al., 2020; Izacard et al., 2022).\\nRetrieval augmented LMs commonly use a\\nretrieve-and-generate setup where they retrieve doc-\\numents based on the user’s input, and then generate\\na complete answer conditioning on the retrieved\\ndocuments (Chen et al., 2017; Guu et al., 2020;\\nLewis et al., 2020; Izacard and Grave, 2021; Sachan\\net al., 2021; Lee et al., 2021; Jiang et al., 2022;\\nIzacard et al., 2022; Nakano et al., 2021; Qian\\net al., 2023; Lazaridou et al., 2022; Shi et al., 2023).\\nThese single-time retrieval augmented LMs outper-\\nform purely parametric LMs, particularly for short-\\nform knowledge-intensive generation tasks such\\nas factoid question answering (QA) (Kwiatkowski\\net al., 2019; Joshi et al., 2017), where the informa-\\ntion needs are clear in the user’s input, and it is\\nsufficient to retrieve relevant knowledge once solely\\nbased on the input .\\nIncreasingly powerful large LMs have also\\ndemonstrated abilities in more complex tasks that\\ninvolve generating long-form output, such as long-\\nform QA (Fan et al., 2019; Stelmakh et al., 2022),\\nopen-domain summarization (Cohen et al., 2021;\\nHayashi et al., 2021; Giorgi et al., 2022), and\\n(chain-of-thought; CoT) reasoning (Wei et al.,\\n2022; Ho et al., 2020; Geva et al., 2021; Hendrycks\\net al., 2020). In contrast to short-form generation,\\nlong-form generation presents complex informa-\\ntion needs that are not always evident from the in-\\nput alone . Similar to how humans gradually gather\\ninformation as we create content such as papers,\\nessays, or books, long-form generation with LMs\\nwould require gathering multiple pieces of knowl-\\nedge throughout the generation process . For ex-\\nample, to generate a summary about a particular\\ntopic, the initial retrieval based on the topic namearXiv:2305.06983v2  [cs.CL]  22 Oct 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='41b644e2-47de-4fab-9f14-26b91c20804b', embedding=None, metadata={'page_label': '2', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generate a summary about Joe Biden.Search results:   !![1]: …[2]: …\\nJoe Biden (born November 20, 1942) is the 46th president of the United States.Joe Biden (born November 20, 1942) is the 46th president of the United States.He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.Joe Biden attended the University of Pennsylvania, where he earned a law degree.RetrieverInputStep 1Search results:   !\"![1]: …[2]: …\"####\"#$#$$Step 2\\nJoe Biden announced his candidacy for the 2020 presidential election on April 25, 2019.Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019.\"#%#%Step 3Search results:   !\"\"[1]: …[2]: …RetrieveddocumentsLMGeneration$%$%%\\nFigure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\\ninput xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\\nitalic ) and check whether it contains low-probability tokens (indicated with underline ). If so (step 2 and 3), the\\nsystem retrieves relevant documents and regenerates the sentence.\\n(e.g., Joe Biden) may not cover all aspects and de-\\ntails. It is crucial to retrieve extra information as\\nneeded during generation, such as when generat-\\ning a certain aspect (e.g., Joe Biden’s education\\nhistory) or a specific detail (e.g., the date of Joe\\nBiden’s presidential campaign announcement).\\nSeveral attempts have been made to retrieve mul-\\ntiple times throughout generation. These attempts\\ninclude methods that passively use the past context\\nto retrieve additional information at a fixed interval\\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\\nRam et al., 2023; Trivedi et al., 2022) which might\\nnot accurately reflect what LMs intend to gener-\\nate in the future or retrieve at inappropriate points.\\nSome works in multihop QA decompose the full\\nquestion into sub-questions, each of which is used\\nto retrieve extra information (Press et al., 2022; Yao\\net al., 2022; Khot et al., 2022; Khattab et al., 2022).\\nWe ask the following question: can we create a\\nsimple and generic retrieval augmented LM that ac-\\ntively decides when and what to retrieve throughout\\nthe generation process, and are applicable to a va-\\nriety of long-form generation tasks? We provide a\\ngeneralized view of active retrieval augmented gen-\\neration. Our hypothesis regarding when to retrieve\\nis that LMs should retrieve information only whenthey lack the required knowledge to avoid unneces-\\nsary or inappropriate retrieval that occurs in passive\\nretrieval augmented LMs (Khandelwal et al., 2020;\\nBorgeaud et al., 2022; Ram et al., 2023; Trivedi\\net al., 2022). Given the observation that large LMs\\ntend to be well-calibrated and low probability/con-\\nfidence often indicates a lack of knowledge (Ka-\\ndavath et al., 2022), we adopt an active retrieval\\nstrategy that only retrieves when LMs generate low-\\nprobability tokens. When deciding what to retrieve ,\\nit is important to consider what LMs intend to gen-\\nerate in the future, as the goal of active retrieval is to\\nbenefit future generations. Therefore, we propose\\nanticipating the future by generating a temporary\\nnext sentence, using it as a query to retrieve rel-\\nevant documents, and then regenerating the next\\nsentence conditioning on the retrieved documents.\\nCombining the two aspects, we propose Forward-\\nLooking Active REtrieval augmented generation\\n(FLARE ), as illustrated in Figure 1. FLARE iter-\\natively generates a temporary next sentence , use\\nit as the query to retrieve relevant documents if it\\ncontains low-probability tokens and regenerate the\\nnext sentence until reaches the end.\\nFLARE is applicable to any existing LMs at\\ninference time without additional training. Con-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa21d9ea-673c-411c-acc4-ca1a2cc237be', embedding=None, metadata={'page_label': '3', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='sidering the impressive performance achieved by\\nGPT-3.5 (Ouyang et al., 2022) on a variety of\\ntasks, we examine the effectiveness of our meth-\\nods on text-davinci-003 . We evaluate FLARE\\non 4 diverse tasks/datasets involving generating\\nlong outputs, including multihop QA (2WikiMul-\\ntihopQA), commonsense reasoning (StrategyQA),\\nlong-form QA (ASQA), and open-domain summa-\\nrization (WikiAsp) (Ho et al., 2020; Geva et al.,\\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\\nOver all tasks, FLARE achieves superior or com-\\npetitive performance compared to single-time and\\nmulti-time retrieval baselines, demonstrating the\\neffectiveness and generalizability of our method.\\n2 Retrieval Augmented Generation\\nWe formally define single-time retrieval augmented\\ngeneration and propose the framework of active\\nretrieval augmented generation.\\n2.1 Notations and Definitions\\nGiven a user input xand a document corpus D=\\n{di}|D|\\ni=1(such as all Wikipedia articles), the goal of\\nretrieval augmented LMs is to generate the answer\\ny= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing\\nmsentences or ntokens leveraging information\\nretrieved from the corpus.\\nIn retrieval augmented LM, the LM typically\\npairs with a retriever that can retrieve a list of\\ndocuments Dq=ret(q)for a query q; the LM\\nconditions on both the user input xand retrieved\\ndocuments Dqto generate the answer. Since we\\nfocus on examining various methods of determin-\\ning when and what to retrieve, we follow exist-\\ning methods (Ram et al., 2023; Trivedi et al.,\\n2022) to prepend the retrieved documents before\\nthe user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny=LM([Dq,x]), where [·,·]is concatenation fol-\\nlowing the specified order.\\n2.2 Single-time Retrieval Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y=LM([Dx,x]).\\n2.3 Active Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t≥1), the retrieval query\\nqtis formulated based on both the user input xand\\npreviously generated output y<t= [y0, ...,yt−1]:\\nqt=qry(x,y<t),\\nwhere qry(·)is the query formulation function. At\\nthe beginning ( t= 1), the previous generation is\\nempty ( y<1=∅), and the user input is used as the\\ninitial query ( q1=x). Given retrieved documents\\nDqt, LMs continually generate the answer until the\\nnext retrieval is triggered or reaches the end:\\nyt=LM([Dqt,x,y<t]),\\nwhere ytrepresents the generated tokens at the cur-\\nrent step t, and the input to LMs is the concatena-\\ntion of the retrieved documents Dqt, the user input\\nx, and the previous generation y<t. We discard\\npreviously retrieved documents ∪t′<tDqt′and only\\nuse the retrieved documents from the current step\\nto condition the next generation to prevent reaching\\nthe input length limit of LMs.\\n3 FLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should reflect\\nthe intents of future generations. We propose two\\nforward-looking active retrieval augmented gener-\\nation (FLARE) methods to implement the active\\nretrieval augmented generation framework. The\\nfirst method prompts the LM to generate retrieval\\nqueries when necessary while generating the an-\\nswer using retrieval-encouraging instructions, de-\\nnoted as FLARE instruct . The second method directly\\nuses the LM’s generation as search queries, denoted\\nas FLARE direct, which iteratively generates the next\\nsentence to gain insight into the future topic, and\\nif uncertain tokens are present, retrieves relevant\\ndocuments to regenerate the next sentence.\\n3.1 FLARE with Retrieval Instructions\\nInspired by Toolformer (Schick et al., 2023), a\\nstraightforward way of expressing information\\nneeds for retrieval is to generate “[Search(query)]”\\nwhen additional information is needed (Schick\\net al., 2023), e.g., “The colors on the flag of\\nGhana have the following meanings. Red is for\\n[Search(Ghana flag red meaning)] the blood of mar-\\ntyrs, ...” When working with GPT-3.5 models that', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b48a7ea-63d9-4aaa-ad22-f7e5ec39c136', embedding=None, metadata={'page_label': '4', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Search results:   !![1]: …[2]: …Joe Biden attended\\nSearch results:   !\"![1]: …[2]: …Search results:   !\"\"[1]: …[2]: …[Search(Joe Biden University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLARE instruct ). It iteratively generates search queries\\n(shown in gray italic ) to retrieve relevant information to\\naid future generations.\\noffer only API access, we elicit such behavior by\\nfew-shot prompting (Brown et al., 2020).\\nSpecifically, for a downstream task, we place\\nthe search-related instruction and exemplars at the\\nbeginning as skill 1, followed by the instruction and\\nexemplars of the downstream task as skill 2. Given\\na test case, we ask LMs to combine skills 1 and 2 to\\ngenerate search queries while performing the task.\\nThe structure of the prompt is shown in Prompt 3.1,\\nand full details can be found in Prompt D.3.\\nPrompt 3.1: retrieval instructions\\nSkill 1. An instruction to guide LMs to generate search\\nqueries.\\nSeveral search-related exemplars.\\nSkill 2. An instruction to guide LMs to perform a\\nspecific downstream task (e.g., multihop QA).\\nSeveral task-related exemplars.\\nAn instruction to guide LMs to combine skills 1\\nand 2 for the test case.\\nThe input of the test case.\\nAs shown in Figure 2, when the LM generates\\n“[Search(query)]” (shown in gray italic ), we stop\\nthe generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the\\nnext search query is generated or reaches the end.\\nAdditional implementation details are included in\\nAppendix A.\\n3.2 Direct FLARE\\nSince we cannot fine-tune black-box LMs, we\\nfound queries generated by FLARE instruct throughretrieval instructions might not be reliable. There-\\nfore, we propose a more direct way of forward-\\nlooking active retrieval that uses the next sentence\\nto decide when and what to retrieve.\\n3.2.1 Confidence-based Active Retrieval\\nAs shown in Figure 1, at step t, we first generate a\\ntemporary next sentence ˆst=LM([x,y<t])with-\\nout conditioning on retrieved documents. Then we\\ndecide whether to trigger retrieval and formulate\\nqueries based on ˆst. If the LM is confident about ˆst,\\nwe accept it without retrieving additional informa-\\ntion; if not, we use ˆstto formulate search queries\\nqtto retrieve relevant documents, and then regen-\\nerate the next sentence st. The reason we utilize\\nsentences as the basis of our iteration is due to their\\nsignificance as semantic units that are neither too\\nshort nor too lengthy like phrases and paragraphs.\\nHowever, our approach can also utilize phrases or\\nparagraphs as the basis.\\nSince LMs tend to be well-calibrated that low\\nprobability/confidence often indicates a lack of\\nknowledge (Jiang et al., 2021; Kadavath et al.,\\n2022; Varshney et al., 2022), we actively trigger\\nretrieval if any token of ˆsthas a probability lower\\nthan a threshold θ∈[0,1].θ= 0means retrieval\\nis never triggered, while θ= 1 triggers retrieval\\nevery sentence.\\nyt=(\\nˆst if all tokens of ˆsthave probs ≥θ\\nst=LM([Dqt,x,y<t]) otherwise\\nwhere the query qtis formulated based on ˆst.\\n3.2.2 Confidence-based Query Formulation\\nOne way to perform retrieval is to directly use the\\nnext sentence ˆstas the query qt. This shares a sim-\\nilar spirit with methods that use generated hypo-\\nthetical titles or paragraphs from LMs as retrieval\\nqueries or evidences (Gao et al., 2022; Sun et al.,\\n2022; Yu et al., 2022; Mao et al., 2021). We gen-\\neralize such techniques to long-form generation\\nwhere active information access is essential.\\nWe found retrieving with the next sentence\\nachieves significantly better results than with the\\nprevious context, as shown later in subsection 6.2.\\nHowever, it has a risk of perpetuating errors con-\\ntained in it. For example, if the LM produces the\\nsentence “Joe Biden attended the University of\\nPennsylvania” instead of the correct fact that he\\nattended the University of Delaware, using this er-\\nroneous sentence as a query might retrieve mislead-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25070538-7a86-4fe7-b482-f40debbcb184', embedding=None, metadata={'page_label': '5', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Joe Biden attended the University of Pennsylvania, where he earned a law degree.Ask a question to which the answer is “the University of Pennsylvania”Ask a question to which the answer is “a law degree”What university did Joe Biden attend?What degree did Joe Biden earn?implicit query by maskingexplicit query by question generationJoe Biden attended  , where he earned  .LM such as ChatGPTFigure 3: Implicit and explicit query formulation. To-\\nkens with low probabilities are marked with underlines .\\ning information. We propose two simple methods\\nto overcome this issue as illustrated in Figure 3.\\nMasked sentences as implicit queries. The first\\nmethod masks out low-confidence tokens in ˆstwith\\nprobabilities below a threshold β∈[0,1], where a\\nhigher βresults in more aggressive masking. This\\nremoves potential distractions from the sentence to\\nimprove retrieval accuracy.\\nGenerated questions as explicit queries. An-\\nother method is to generate explicit questions that\\ntarget the low-confident span in ˆst. For example, if\\nthe LM is uncertain about “the University of Penn-\\nsylvania”, a question like “Which university did\\nJoe Biden attend?” can help retrieve relevant in-\\nformation. Self-ask (Press et al., 2022) achieved\\nthis by manually inserting follow-up questions\\ninto downstream task exemplars as shown later\\nin Prompt D.2, which requires task-specific annota-\\ntion efforts. Instead, we developed a universal ap-\\nproach that generates questions for low-confidence\\nspans without additional annotation. Specifically,\\nWe first extract all spans from ˆstwith probabilities\\nbelow β. For each extracted span z, we prompt\\ngpt-3.5-turbo to generate a question qt,zthat\\ncan be answered with the span:\\nPrompt 3.2: zero-shot question generation\\nUser input x.\\nGenerated output so far y≤t.\\nGiven the above passage, ask a question to which\\nthe answer is the term/entity/phrase “ z”.\\nWe retrieve using each generated question and\\ninterleave the returned documents into a single\\nranking list to aid future generations. In summary,queries qtare formulated based on ˆstas follows:\\nqt=(\\n∅ if all tokens of ˆsthave probs ≥θ\\nmask(ˆst)or qgen (ˆst) otherwise\\n3.3 Implementation Details\\nBase LM We validate our method on one of the\\nmost advanced GPT-3.5 LMs text-davinci-003\\nby iteratively querying their API.2\\nDocument corpus and retrievers. Since we fo-\\ncus on the integration of retrieval and generation,\\nwe use off-the-shelf retrievers that take queries\\nas inputs and return a list of relevant documents.\\nFor datasets that mainly rely on knowledge from\\nWikipedia, we use the Wikipedia dump from\\nKarpukhin et al. (2020) and employ BM25 (Robert-\\nson and Zaragoza, 2009) as the retriever. For\\ndatasets that rely on knowledge from the open web,\\nwe use the Bing search engine as our retriever.3\\nRetrieved document formatting. Multiple re-\\ntrieved documents are linearized according to their\\nranking and then added to the beginning of the user\\ninput using Prompt D.1.\\nOther implementation details such as sentence to-\\nkenization and efficiency are included Appendix A.\\n4 Multi-time Retrieval Baselines\\nExisting passive multi-time retrieval augmented\\nLMs can also be formulated using our framework\\n(subsection 2.3). In this section, we formally in-\\ntroduce three baseline categories based on when\\nand what to retrieve. These baselines are not exact\\nreproductions of the corresponding paper because\\nmany design choices differ which makes direct\\ncomparisons impossible. We implemented them\\nusing the same settings, with the only variation\\nbeing when and what to retrieve.\\nPrevious-window approaches trigger retrieval\\nevery ltokens, where lrepresents the window size.\\nGenerated tokens from the previous window are\\nused as the query:\\nqt=yt−1(t≥2),\\nyt= [w(t−1)l+1, ..., w tl].\\nSome existing methods in this category are RETRO\\n(Borgeaud et al., 2022), IC-RALM (Ram et al.,\\n2https://api.openai.com/v1/completions April 23.\\n3https://www.microsoft.com/en-us/bing/apis/\\nbing-web-search-api', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d65c3341-d593-4fa3-b6a9-cbee8a6c07c8', embedding=None, metadata={'page_label': '6', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2023), which retrieve every few tokens, and KNN-\\nLM (Khandelwal et al., 2020), which retrieves ev-\\nery token.4We follow Ram et al. (2023) to use a\\nwindow size of l= 16 .\\nPrevious-sentence approaches trigger retrieval\\nevery sentence and use the previous sentence as the\\nquery, and IRCoT (Trivedi et al., 2022) belongs to\\nthis category:\\nqt=yt−1(t≥2),\\nyt=st.\\nQuestion decomposition approaches manually\\nannotated task-specific exemplars to guide LMs\\nto generate decomposed sub-questions while pro-\\nducing outputs. For example, self-ask (Press et al.,\\n2022), a method in this category, manually inserts\\nsub-questions in exemplars using Prompt D.2. For\\nthe test case, retrieval is triggered dynamically\\nwhenever the model generates a sub-question.\\nThe aforementioned approaches can retrieve ad-\\nditional information while generating. However,\\nthey have notable drawbacks: (1) Using previously\\ngenerated tokens as queries might not reflect what\\nLMs intend to generate in the future. (2) Retriev-\\ning information at a fixed interval can be inefficient\\nbecause it might occur at inappropriate points. (3)\\nQuestion decomposition approaches require task-\\nspecific prompt engineering, which restricts their\\ngeneralizability in new tasks.\\n5 Experimental Setup\\nWe evaluate the effectiveness of FLARE on 4 di-\\nverse knowledge-intensive tasks using few-shot in-\\ncontext learning (Radford et al., 2019; Brown et al.,\\n2020; Liu et al., 2023). We follow previous works\\n(Trivedi et al., 2022) to sub-sample at most 500\\nexamples from each dataset due to the cost of run-\\nning experiments. Datasets, metrics, and settings\\nare summarized in Table 7 of Appendix B. The\\nhyperparameters of FLARE are selected based on\\nthe development set and listed in Table 9. FLARE\\nrefers to FLARE direct if not specifically stated.\\nMultihop QA The goal of multihop QA is to\\nanswer complex questions through information re-\\ntrieval and reasoning. We use 2WikiMultihopQA\\n(Ho et al., 2020) which contains 2-hop complex\\n4Since KNN-LM uses the contextualized representation\\ncorresponding to the current decoding position to retrieve rel-\\nevant information which encodes all previous tokens. Strictly\\nspeaking, qtshould be y<t.questions sourced from Wikipedia articles that re-\\nquire composition, comparison, or inference, e.g.,\\n“Why did the founder of Versus die?” We follow\\nWang et al. (2022) to generate both the chain-of-\\nthought and the final answer. Experimental setting\\ndetails are included in Appendix B.\\nWe use regular expressions to extract the final\\nanswer from the output and compare it with the ref-\\nerence answer using exact match (EM), and token-\\nlevel F 1, precision, and recall.\\nCommonsense reasoning Commonsense reason-\\ning requires world and commonsense knowledge\\nto generate answers. We use StrategyQA (Geva\\net al., 2021) which is a collection of crowdsourced\\nyes/no questions, e.g., “Would a pear sink in wa-\\nter?” We follow Wei et al. (2022) to generate both\\nthe chain-of-thought and the final yes/no answer.\\nDetails are included in Appendix B.\\nWe extract the final answer and match it against\\nthe gold answer using exact match.\\nLong-form QA Long-form QA aims to generate\\ncomprehensive answers to questions seeking com-\\nplex information (Fan et al., 2019; Stelmakh et al.,\\n2022). We use ASQA (Stelmakh et al., 2022) as our\\ntestbed where inputs are ambiguous questions with\\nmultiple interpretations, and outputs should cover\\nall of them. For example, “Where do the Philadel-\\nphia Eagles play their home games?” could be\\nasking about the city, sports complex, or stadium.\\nWe found in many cases it is challenging even for\\nhumans to identify which aspect of the question\\nis ambiguous. Therefore, we created another set-\\nting (ASQA-hint) where we provide a brief hint\\nto guide LMs to stay on track when generating an-\\nswers. The hint for the above case is “This question\\nis ambiguous in terms of which specific location or\\nvenue is being referred to.” Experimental setting\\ndetails are included in Appendix B.\\nWe use metrics from Stelmakh et al. (2022), in-\\ncluding EM, RoBERTa-based QA score (Disambig-\\nF1), ROUGE (Lin, 2004), and an overall score com-\\nbining Disambig-F 1and ROUGE (DR).\\nOpen-domain summarization The goal of open-\\ndomain summarization is to generate a comprehen-\\nsive summary about a topic by gathering informa-\\ntion from open web (Giorgi et al., 2022). We use\\nWikiAsp (Hayashi et al., 2021) which aims to gen-\\nerate aspect-based summaries about entities from\\n20 domains in Wikipedia, e.g., “Generate a sum-\\nmary about Echo School (Oregon) including the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b78096b-55a8-42b9-bdd8-5d5b403e8734', embedding=None, metadata={'page_label': '7', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='0.020.040.060.080.0\\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAspNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\\nfollowing aspects: academics, history.” Experimen-\\ntal setting details are included in Appendix B.\\nMetrics include ROUGE, named entity-based F 1,\\nand UniEval (Zhong et al., 2022) which measures\\nfactual consistency.\\n6 Experimental Results\\nWe first report overall results across 4 tasks/datasets\\nand compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efficacy of\\nvarious design choices of our method.\\n6.1 Comparison with Baselines\\nOverall results. The overall performance of\\nFLARE and baseline across all tasks/datasets are\\nreported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks, multihop QA shows the\\nmost significant improvement. This is largely due\\nto the task’s clear definition and specific objective\\nof producing the final answer through a 2-hop rea-\\nsoning process, which makes it easier for LMs to\\ngenerate on-topic output. In contrast, ASQA and\\nWikiAsp are more open-ended, which increases the\\ndifficulty of both generation and evaluation. The\\nimprovement on ASQA-hint is larger than that of\\nASQA because identifying ambiguous aspects is\\nchallenging even for humans in many cases, and\\nproviding a generic hint helps LMs to stay on topic.\\nThorough comparisons with baselines. The per-\\nformance of all baselines on 2WikiMultihopQA\\nare reported in Table 1. FLARE outperforms all\\nbaselines by a large margin, which confirms that\\nforward-looking active retrieval is highly effective.\\nMost multi-time retrieval augmented approaches\\noutperform single-time retrieval but with differentMethods EM F 1 Prec. Rec.\\nNo retrieval 28.2 36.8 36.5 38.6\\nSingle-time retrieval 39.4 48.8 48.6 51.5\\nMulti-time retrieval\\nPrevious-window 43.2 52.3 51.7 54.5\\nPrevious-sentence 39.0 49.2 48.9 51.8\\nQuestion decomposition 47.8 56.4 56.1 58.6\\nFLARE instruct (ours) 42.4 49.8 49.1 52.5\\nFLARE direct(ours) 51.0 59.7 59.1 62.6\\nTable 1: FLARE and baselines on 2WikiMultihopQA.\\nPrevious-window (Borgeaud et al., 2022; Ram et al.,\\n2023), previous-sentence (Trivedi et al., 2022), and ques-\\ntion decomposition (Press et al., 2022; Yao et al., 2022)\\nmethods are reimplemented for fair comparisons.\\nmargins. The improvement of retrieving using the\\nprevious sentence is relatively small which we hy-\\npothesize is mainly because the previous sentence\\noften describes entities or relations different from\\nthose in the next sentence in 2WikiMultihopQA.\\nWhile the previous-window approach might use\\nthe first half of a sentence to retrieve information\\npotentially helpful for generating the second half.\\nAmong all baselines, the question decomposition\\napproach (Press et al., 2022) achieves the best per-\\nformance. which is not surprising since the in-\\ncontext exemplars manually annotated with decom-\\nposed sub-questions (Prompt D.2) guide LMs to\\ngenerate sub-questions that align with the topic/in-\\ntent of future generations. FLARE outperforms\\nthis baseline, indicating that manual exemplar an-\\nnotation is not necessary for effective future-aware\\nretrieval. The gap between FLARE instruct and ques-\\ntion decomposition is large, indicating that teaching\\nLMs to generate search queries using task-generic\\nretrieval instructions and exemplars is challenging.\\nWe report all metrics for the other datasets in\\nTable 2. FLARE outperforms baselines with re-\\nspect to all metrics. Retrieval using the previ-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d1e3a947-db5a-4d1c-a5ba-e42755760f0e', embedding=None, metadata={'page_label': '8', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Datasets StrategyQA ASQA ASQA-hint WikiAsp\\nMetrics EM EM D-F 1R-L DR EM D-F 1R-L DR UniEval E-F 1R-L\\nNo retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4\\nSingle-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9\\nMulti-time retrieval\\nPrevious-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3\\nPrevious-sentence 71.0 39.9 27.9 34.3 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\\nFLARE (ours) 77.3 41.3 28.2 34.3 31.1 46.2 36.7 37.7 37.2 53.4 18.9 27.6\\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F 1is\\nDisambig-F 1, R-L is ROUGE-L, and E-F 1is named entity-based F 1.\\n2WikiMultihopQA ASQA-hint\\nEM F 1Prec. Rec. EM D-F 1R-L DR\\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\\nTable 3: A head-to-head comparison between using the\\nprevious sentence and the next sentence for retrieval.\\n#Tokens EM F 1 Prec. Rec.\\n16 43.2 52.3 51.7 54.5\\n32 43.6 52.4 52.0 55.0\\n48 40.0 49.3 49.0 52.0\\nAll 39.0 48.5 48.2 51.1\\nTable 4: Previous-window approaches using different\\nnumbers of tokens as queries.\\nous window underperforms single-time retrieval\\non ASQA, which we hypothesize is because the\\nprevious window does not accurately reflect future\\nintent. Since we focus on evaluating factuality, met-\\nrics with an emphasis on factual content (such as\\nEM, Disambig-F 1, UniEval) are more reliable than\\nmetrics computed over all tokens (ROUGE-L).\\n6.2 Ablation Study\\nImportance of forward-looking retrieval. We\\nfirst validate that forward-looking retrieval is more\\neffective than past-context-based retrieval. We run\\nablation experiments on 2WikiMultihopQA and\\nASQA-hint comparing retrieval using the previ-\\nous versus the next sentence. Specifically, both\\nmethods retrieve every sentence and directly use\\nthe complete previous/next sentence as queries. As\\nshown in Table 3, using the next sentence to retrieve\\nis clearly better than using the previous sentence,\\nconfirming our hypothesis.\\nWe also run previous-window approaches using\\ndifferent numbers of past tokens as queries. As\\nshown in Table 4, using too many tokens ( >32) in\\n%steps/sentences with retrieval0.020.040.060.080.0\\n0.0 25.0 50.0 75.0 100.02WikiMultihopQA StrategyQAFigure 5: Performance (EM) of FLARE with respect\\nto the percentage of steps/sentences with retrieval on\\n2WikiMultihopQA and StrategyQA.\\nthe past hurts the performance, further confirming\\nour hypothesis that previous context might not be\\nrelevant to intent of future generations.\\nImportance of active retrieval. Next, we inves-\\ntigate how active retrieval threshold θaffects per-\\nformance. To alter our method from not retrieving\\nto retrieving every sentence, we adjust the confi-\\ndence threshold θthat determines when to trigger\\nretrieval from 0 to 1. We then calculate the pro-\\nportion of steps/sentences where retrieval is acti-\\nvated, and present the performance based on it. As\\nshown in Figure 5, on 2WikiMultihopQA, the per-\\nformance plateaus when the retrieval percentage\\nexceeds 60%, indicating that retrieval when LMs\\nare confident is not necessary. On StrategyQA, the\\nperformance drops when the retrieval percentage\\nexceeds 50%, indicating that unnecessary retrieval\\ncan introduce noise and impede the original gen-\\neration process. We found triggering retrieval for\\n40%-80% of sentences usually leads to a good per-\\nformance across tasks/datasets.\\nEffectiveness of different query formulation\\nmethods We study implicit query formation by\\nmasking and explicit query formulation through\\nquestion generation. In Table 5, we compare the\\nperformance of FLARE with different masking', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='325600be-54e8-4f26-a7b8-7abde8314c85', embedding=None, metadata={'page_label': '9', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='β EM F 1 Prec. Rec.\\n0.0 0.488 0.576 0.571 0.605\\n0.2 0.498 0.588 0.582 0.616\\n0.4 0.510 0.597 0.591 0.627\\n0.6 0.506 0.593 0.586 0.622\\nTable 5: Performance of FLARE with respect to the\\nmasking threshold βon 2WikiMultihopQA.\\nASQA-hint WikiAsp\\nEM D-F 1R-L DR UniEval E-F 1R-L\\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\\nTable 6: A comparison between implicit and explicit\\nquery formulation methods in FLARE.\\nthresholds β. Retrieving directly with the complete\\nsentence ( β= 0) is worse than masking tokens\\nwith low probabilities, confirming our hypothesis\\nthat low-confidence erroneous tokens can distract\\nretrievers. We compare implicit and explicit query\\nformulation methods in Table 6. Performances of\\nboth methods are similar, indicating that both meth-\\nods can effectively reflect information needs.\\n7 Related Work\\nWe refer to subsection 2.2 and section 4 for ex-\\ntensively discussion on single-time and multi-time\\nretrieval augmented LMs, which is the most rele-\\nvant area to this paper.\\nIterative and adaptive retrieval Iterative re-\\ntrieval and refinement has been studied in both\\ntext and code generation tasks (Peng et al., 2023;\\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\\net al., 2023). FLARE differs from these methods in\\nthe granularity of generation and retrieval strategies.\\nAdaptive retrieval has been studied in single-time\\nretrieval scenarios based on either question pop-\\nularity or generation probabilities (Mallen et al.,\\n2022; Li et al., 2023), while we focus on long-form\\ngeneration requiring active information access.\\nBrowser-enhanced LMs WebGPT (Nakano\\net al., 2021) and WebCPM (Qin et al., 2023) train\\nLMs to interact with browser to enhance factuality\\nusing reinforcement learning or supervised train-\\ning where multiple queries can be triggered before\\ngeneration. FLARE is built on text-based retrievers\\nbut can be combined with a browser to potentially\\nimprove retrieval quality.8 Conclusion\\nTo aid long-form generation with retrieval aug-\\nmentation, we propose an active retrieval aug-\\nmented generation framework that decides when\\nand what to retrieve during generation. We imple-\\nment this framework with forward-looking active\\nretrieval that iteratively uses the upcoming sentence\\nto retrieve relevant information if it contains low-\\nconfidence tokens and regenerates the next sen-\\ntence. Experimental results on 4 tasks/datasets\\ndemonstrate the effectiveness of our methods. Fu-\\nture directions include better strategies for active\\nretrieval and developing efficient LM architectures\\nfor active information integration.\\n9 Limitations\\nWe also conduct experiments on Wizard of\\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\\n2019), and found that FLARE did not provide sig-\\nnificant gains. Wizard of Wikipedia is a knowledge-\\nintensive dialogue generation dataset where the out-\\nput is relatively short ( ∼20 tokens on average) so\\nretrieving multiple disparate pieces of information\\nmight not be necessary. ELI5 (Fan et al., 2019)\\nis a long-form QA dataset requiring in-depth an-\\nswers to open-ended questions. Due to issues men-\\ntioned in Krishna et al. (2021) such as difficulties\\nof grounding generation in retrieval and evalua-\\ntion, both single-time retrieval and FLARE did not\\nprovide significant gains over not using retrieval.\\nFrom an engineering perspective, interleaving gen-\\neration and retrieval with a naive implementation\\nincreases both overheads and the cost of generation.\\nLMs need to be activated multiple times (once for\\neach retrieval) and a caching-free implementation\\nalso requires recomputing the previous activation\\neach time after retrieval. This issue can be poten-\\ntially alleviated with special architectural designs\\nthat encode the retrieved documents Dqtand the\\ninput/generation ( x/y<t) independently.\\nAcknowledgements\\nThis work was supported in part by a grant from\\nthe Singapore Defence Science and Technology\\nAgency and the IBM PhD Fellowship. We thank\\nChunting Zhou, Amanda Bertsch, Uri Alon, Hi-\\nroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo\\nSchick, Kaixin Ma, Shuyan Zhou, and Songwei Ge\\nfor their insightful discussions and help with the\\nexperiments.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d240d25-c197-4772-b17e-8db547c623ba', embedding=None, metadata={'page_label': '10', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA , volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, ACL 2017, Vancouver, Canada, July 30 -\\nAugust 4, Volume 1: Long Papers , pages 1870–1879.\\nAssociation for Computational Linguistics.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2022. Palm: Scaling language mod-\\neling with pathways. CoRR , abs/2204.02311.Nachshon Cohen, Oren Kalinsky, Yftah Ziser, and\\nAlessandro Moschitti. 2021. Wikisum: Coherent\\nsummarization dataset for efficient human-evaluation.\\nInProceedings of the 59th Annual Meeting of the As-\\nsociation for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language\\nProcessing, ACL/IJCNLP 2021, (Volume 2: Short\\nPapers), Virtual Event, August 1-6, 2021 , pages 212–\\n219. Association for Computational Linguistics.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2019. Wizard\\nof wikipedia: Knowledge-powered conversational\\nagents. In 7th International Conference on Learning\\nRepresentations, ICLR 2019, New Orleans, LA, USA,\\nMay 6-9, 2019 . OpenReview.net.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\nier, Jason Weston, and Michael Auli. 2019. ELI5:\\nlong form question answering. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers , pages\\n3558–3567. Association for Computational Linguis-\\ntics.\\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\\n2022. Precise zero-shot dense retrieval without rele-\\nvance labels. CoRR , abs/2212.10496.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics , 9:346–\\n361.\\nJohn M. Giorgi, Luca Soldaini, Bo Wang, Gary D.\\nBader, Kyle Lo, Lucy Lu Wang, and Arman Co-\\nhan. 2022. Exploring the challenges of open\\ndomain multi-document summarization. CoRR ,\\nabs/2212.10526.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\naugmented language model pre-training. CoRR ,\\nabs/2002.08909.\\nHiroaki Hayashi, Prashant Budania, Peng Wang, Chris\\nAckerson, Raj Neervannan, and Graham Neubig.\\n2021. Wikiasp: A dataset for multi-domain aspect-\\nbased summarization. Trans. Assoc. Comput. Lin-\\nguistics , 9:211–225.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2020. Measuring massive multitask language under-\\nstanding. CoRR , abs/2009.03300.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing A multi-hop\\nQA dataset for comprehensive evaluation of reason-\\ning steps. In Proceedings of the 28th International\\nConference on Computational Linguistics, COLING\\n2020, Barcelona, Spain (Online), December 8-13,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47980e14-f1a7-4e46-a652-ad4885e782d2', embedding=None, metadata={'page_label': '11', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2020 , pages 6609–6625. International Committee on\\nComputational Linguistics.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume,\\nEACL 2021, Online, April 19 - 23, 2021 , pages 874–\\n880. Association for Computational Linguistics.\\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2022. Few-shot learning with\\nretrieval augmented language models. CoRR ,\\nabs/2208.03299.\\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\\nNeubig. 2021. How can we know When language\\nmodels know? on the calibration of language mod-\\nels for question answering. Trans. Assoc. Comput.\\nLinguistics , 9:962–977.\\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\\nZhiruo Wang, Jamie Callan, and Graham Neubig.\\n2022. Retrieval as attention: End-to-end learning\\nof retrieval and reading within a single transformer.\\nCoRR , abs/2212.02027.\\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\\nNeubig. 2020. How can we know what language\\nmodels know. Trans. Assoc. Comput. Linguistics ,\\n8:423–438.\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics, ACL\\n2017, Vancouver, Canada, July 30 - August 4, Volume\\n1: Long Papers , pages 1601–1611. Association for\\nComputational Linguistics.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\\nBen Mann, Sam McCandlish, Chris Olah, and Jared\\nKaplan. 2022. Language models (mostly) know what\\nthey know. CoRR , abs/2207.05221.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020 , pages 6769–6781. Associa-\\ntion for Computational Linguistics.Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In 8th International Conference on Learning\\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\\nApril 26-30, 2020 . OpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\\nDavid Hall, Percy Liang, Christopher Potts, and\\nMatei Zaharia. 2022. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive NLP. CoRR , abs/2212.14024.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\\n2022. Decomposed prompting: A modular approach\\nfor solving complex tasks. CoRR , abs/2210.02406.\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\\nHurdles to progress in long-form question answering.\\nInNorth American Association for Computational\\nLinguistics .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Trans. Assoc. Comput. Linguistics , 7:452–\\n466.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\\nStokowiec, and Nikolai Grigorev. 2022. Internet-\\naugmented language models through few-shot\\nprompting for open-domain question answering.\\nCoRR , abs/2203.05115.\\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\\njape, Christopher D. Manning, and Kyoung-Gu Woo.\\n2021. You only need one model for open-domain\\nquestion answering. CoRR , abs/2112.07381.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in Neu-\\nral Information Processing Systems 33: Annual Con-\\nference on Neural Information Processing Systems\\n2020, NeurIPS 2020, December 6-12, 2020, virtual .\\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\\nJian-Yun Nie, and Ji-Rong Wen. 2023. The web can\\nbe your oyster for improving large language models.\\nCoRR , abs/2305.10998.\\nChin-Yew Lin. 2004. ROUGE: A package for auto-\\nmatic evaluation of summaries. In Text Summariza-\\ntion Branches Out , pages 74–81, Barcelona, Spain.\\nAssociation for Computational Linguistics.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eee5fbc0-053a-4847-bceb-2ca717528e16', embedding=None, metadata={'page_label': '12', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\\ntrain, prompt, and predict: A systematic survey of\\nprompting methods in natural language processing.\\nACM Comput. Surv. , 55(9):195:1–195:35.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nHannaneh Hajishirzi, and Daniel Khashabi. 2022.\\nWhen not to trust language models: Investigating\\neffectiveness and limitations of parametric and non-\\nparametric memories. CoRR , abs/2212.10511.\\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\\n2021. Generation-augmented retrieval for open-\\ndomain question answering. In Proceedings of the\\n59th Annual Meeting of the Association for Compu-\\ntational Linguistics and the 11th International Joint\\nConference on Natural Language Processing, ACL/I-\\nJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\\nAugust 1-6, 2021 , pages 4089–4100. Association for\\nComputational Linguistics.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\\nRyan McDonald. 2020. On faithfulness and factu-\\nality in abstractive summarization. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 1906–1919, On-\\nline. Association for Computational Linguistics.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2021. Webgpt: Browser-\\nassisted question-answering with human feedback.\\nCoRR , abs/2112.09332.\\nOpenAI. 2023. GPT-4 technical report. CoRR ,\\nabs/2303.08774.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. CoRR , abs/2203.02155.\\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\\nyour facts and try again: Improving large language\\nmodels with external knowledge and automated feed-\\nback. CoRR , abs/2302.12813.\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander H. Miller. 2019. Language mod-\\nels as knowledge bases? In Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing,EMNLP-IJCNLP 2019, Hong Kong, China, Novem-\\nber 3-7, 2019 , pages 2463–2473. Association for\\nComputational Linguistics.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2022. Measuring\\nand narrowing the compositionality gap in language\\nmodels. arXiv preprint arXiv:2210.03350 .\\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\\nJian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\\nLearning to generate factually correct articles for\\nqueries by grounding on large web corpus. CoRR ,\\nabs/2304.04358.\\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\\nLiu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-\\nteractive web search for chinese long-form question\\nanswering. CoRR , abs/2305.06849.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nBlog , 1(8).\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. arXiv preprint arXiv:2302.00083 .\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2020, Online, Novem-\\nber 16-20, 2020 , pages 5418–5426. Association for\\nComputational Linguistics.\\nStephen E. Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: BM25 and be-\\nyond. Found. Trends Inf. Retr. , 3(4):333–389.\\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\\nend training of multi-document reader and retriever\\nfor open-domain question answering. In Advances\\nin Neural Information Processing Systems 34: An-\\nnual Conference on Neural Information Processing\\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\\nvirtual , pages 25968–25981.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:\\nLanguage models can teach themselves to use tools.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR , abs/2301.12652.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4eb82fa3-8460-4d6a-9026-eee8bd6fec08', embedding=None, metadata={'page_label': '13', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\nWei Chang. 2022. ASQA: factoid questions meet\\nlong-form answers. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\\nEmirates, December 7-11, 2022 , pages 8273–8288.\\nAssociation for Computational Linguistics.\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\\nDenny Zhou. 2022. Recitation-augmented language\\nmodels. CoRR , abs/2210.01296.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. CoRR ,\\nabs/2302.13971.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\\nKhot, and Ashish Sabharwal. 2022. Interleav-\\ning retrieval with chain-of-thought reasoning for\\nknowledge-intensive multi-step questions. CoRR ,\\nabs/2212.10509.\\nNeeraj Varshney, Man Luo, and Chitta Baral. 2022. Can\\nopen-domain QA reader utilize external knowledge\\nefficiently like humans? CoRR , abs/2211.12707.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\\nconsistency improves chain of thought reasoning in\\nlanguage models. CoRR , abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. CoRR , abs/2201.11903.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\nReact: Synergizing reasoning and acting in language\\nmodels. CoRR , abs/2210.03629.\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\\nMichael Zeng, and Meng Jiang. 2022. Generate\\nrather than retrieve: Large language models are\\nstrong context generators. CoRR , abs/2209.10063.\\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\\nJiang, and Ashish Sabharwal. 2023. Improving lan-\\nguage models via plug-and-play retrieval feedback.\\nCoRR , abs/2305.14002.\\nYury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\\nPanupong Pasupat, Peter Shaw, Linlu Qiu, Sumit\\nSanghai, and Fei Sha. 2022. Generate-and-retrieve:\\nUse your predictions to improve retrieval for seman-\\ntic parsing. In Proceedings of the 29th International\\nConference on Computational Linguistics, COLING\\n2022, Gyeongju, Republic of Korea, October 12-17,\\n2022 , pages 4946–4951. International Committee on\\nComputational Linguistics.Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\\n2023. Repocoder: Repository-level code completion\\nthrough iterative retrieval and generation. CoRR ,\\nabs/2303.12570.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\\nWang, and Luke Zettlemoyer. 2022. Opt: Open\\npre-trained transformer language models. ArXiv ,\\nabs/2205.01068.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\\n2023. A survey of large language models. CoRR ,\\nabs/2303.18223.\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\nJiawei Han. 2022. Towards a unified multi-\\ndimensional evaluator for text generation. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing, EMNLP 2022,\\nAbu Dhabi, United Arab Emirates, December 7-11,\\n2022 , pages 2023–2038. Association for Computa-\\ntional Linguistics.\\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\\nFrancisco Guzmán, Luke Zettlemoyer, and Marjan\\nGhazvininejad. 2021. Detecting hallucinated content\\nin conditional neural sequence generation. In Find-\\nings of the Association for Computational Linguis-\\ntics: ACL-IJCNLP 2021 , pages 1393–1404, Online.\\nAssociation for Computational Linguistics.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6beb5cb4-afd5-4dd5-b171-5015fa51e4d5', embedding=None, metadata={'page_label': '14', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A FLARE Implementation Details\\nFLARE instruct implementation details We\\nfound that LMs can effectively combine retrieval\\nand downstream task-related skills and generate\\nmeaningful search queries while performing the\\ntask. However, there are two issues: (1) LMs tend\\nto generate fewer search queries than necessary.\\n(2) Generating excessive search queries can\\ndisrupt answer generation and adversely affect\\nperformance. We address these issues using two\\nmethods respectively. First, we increase the logit\\nof the token “[” by 2.0 to improve the chances\\nof LMs generating “[Search(query)]”. Second,\\nwhenever LMs generate a search query, we use it\\nto retrieve relevant information, promptly remove\\nit from the generation, and generate the next few\\ntokens while forbidding “[” by adding a large\\nnegative value to the logit of “[”.\\nThe initial query of FLARE. FLARE starts\\nwith the user input xas the initial query to re-\\ntrieve documents to generate the first sentence\\nˆs1=LM([Dx,x])to bootstrap the iterative gener-\\nation process. For the following steps, the tempo-\\nrary forward-looking sentence is generated without\\nretrieved documents.\\nSentence tokenization. For each step t, we gen-\\nerate 64 tokens which are longer than most sen-\\ntences, and use NLTK sentence tokenizer5to ex-\\ntract the first sentence and discard the rest.\\nEfficiency As shown in subsection 6.2, on aver-\\nage retrieval is triggered for 30%∼60% of sen-\\ntences depending on downstream tasks. In compar-\\nision, KNN-LM (Khandelwal et al., 2020) retrieves\\nevery token, RETRO or IC-RALM (Borgeaud et al.,\\n2022; Ram et al., 2023) retrievers every 4 ∼32 to-\\nkens, and IRCoT (Trivedi et al., 2022) retrieves\\nevery sentence. Compared to single-time retrieval,\\nhowever, interleaving retrieval and generation with\\na naive implementation indeed increases overheads,\\nwhich we discuss in the limitation section (sec-\\ntion 9).\\nB Datasets and Settings\\nDatasets, metrics, and experimental settings are\\nsummarized in Table 7.\\n5https://www.nltk.org/api/nltk.tokenize.\\nPunktSentenceTokenizer.htmlMultihop QA For “Why did the founder of Ver-\\nsus die?”, the output we aim to generate is “The\\nfounder of Versus was Gianni Versace. Gianni Ver-\\nsace was shot and killed on the steps of his Miami\\nBeach mansion on July 15, 1997. So the answer\\nis shot.” We use 8 exemplars from Trivedi et al.\\n(2022) listed in Prompt D.4 for in-context learn-\\ning, BM25 as the retriever, and Wikipedia articles\\nas the retrieval corpus. Similar to the observation\\nin Trivedi et al. (2022), we found incorporating\\nretrieval results for exemplars improves the per-\\nformance, we use the input xof each exemplar to\\nretrieve several documents and then add them using\\nthe format in Prompt D.1. We found increasing the\\nnumber of retrieval documents often increases per-\\nformance. Therefore, we use the maximum number\\nof documents that can fit within the input length\\nlimit of text-davinci-003 , which is 2 for 2Wiki-\\nMultihopQA.\\nCommonsense Reasoning For “Would a pear\\nsink in water?”, the output we aim to generate is\\n“The density of a pear is about 0.6g/cm3, which is\\nless than water. Objects less dense than water float.\\nThus, a pear would float. So the final answer is no.”\\nWe use 6 exemplars from Wei et al. (2022) listed in\\nPrompt D.5, BM25 on the Wikipedia corpus, and 3\\nretrieved documents to run experiments.\\nLong-form QA For “Where do the Philadelphia\\nEagles play their home games?”, the output we\\naim to generate is “We need to consider the dif-\\nferent possible locations or venues that could be\\nconsidered the home field of the Philadelphia Ea-\\ngles. These include the city, the sports complex,\\nor the stadium. Therefore, this question has 3 in-\\nterpretations and the answers are: (1) The city is\\nPhiladelphia. (2) The sports complex is the South\\nPhiladelphia Sports Complex. (3) The stadium is\\nthe Lincoln Financial Field stadium.” For both the\\noriginal setting (ASQA) and the setting with hints\\n(ASQA-hint), we manually annotate 8 exemplars\\n(Prompt D.6 and D.8), use BM25 on the Wikipedia\\ncorpus, and 3 retrieved documents to run experi-\\nments.\\nOpen-domain Summarization The original\\nWikiAsp dataset is designed for multi-document\\nsummarization and provides a list of references to\\nsystems. We converted it into the open-domain\\nsetting by removing the associated references and\\ninstead gathering information from the open web.\\nFor “Generate a summary about Echo School (Ore-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d087bcfe-befe-4311-950b-f8fb9e41f44a', embedding=None, metadata={'page_label': '15', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='gon) including the following aspects: academics,\\nhistory.”, the output we aim to generate is “# Aca-\\ndemics. In 2008, 91% of the school’s seniors re-\\nceived their high school diploma... # History. The\\nclass of 2008 was the 100th class in the school’s\\nhistory.” where # is used to indicate aspects. We\\nmanually annotate 4 exemplars (Prompt D.10), and\\nuse the Bing search engine to retrieve 5 documents\\nfrom the open web. To avoid leaking, we exclude\\nseveral Wikipedia-related domains listed in Table 8\\nfrom Bing’s search results.\\nC Hyperparameters\\nHyperparameters of FLARE on different datasets\\nare listed in Table 9.\\nD Prompts and Few-shot exemplars\\nThe prompt used to linearize multiple documents\\nis shown in Prompt D.1. The prompt used in self-\\nask (Press et al., 2022) is shown in Prompt D.2.\\nPrompts and exemplars of different tasks/datasets\\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\\nD.10, respectively.\\nPrompt D.1: document formatting\\nSearch results:\\n[1]Document 1\\n[2]Document 2\\n...\\nThe user input x\\nPrompt D.2: multihop QA with self-ask\\nQuestion: Who lived longer, Theodor Haecker or Harry\\nVaughan Watkins?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Theodor Haecker when he died?\\nIntermediate answer: Theodor Haecker was 65 years old\\nwhen he died.\\nFollow up: How old was Harry Vaughan Watkins when he\\ndied?\\nIntermediate answer: Harry Vaughan Watkins was 69 years\\nold when he died.\\nSo the final answer is: Harry Vaughan Watkins.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9b96306-a388-478e-8f25-6a0977ea3c27', embedding=None, metadata={'page_label': '16', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Settings 2WikiMultihopQA StrategyQA ASQA WikiAsp\\n(Ho et al., 2020) (Geva et al., 2021) (Stelmakh et al., 2022) (Hayashi et al., 2021)\\nDataset statistics\\nTask multihop QA commonsense QA long-form QA open-domain summarization\\n#Examples 500 229 500 500\\nEvaluation settings\\nMetrics EM, F 1, Prec., Rec. EM EM, Disambig-F 1, ROUGE, DR UniEval, entity-F 1, ROUGE\\nRetrieval settings\\nCorpus Wikipedia Wikipedia Wikipedia open web\\nRetriever BM25 BM25 BM25 Bing\\nTop-k 2 3 3 5\\nPrompt format\\n#Exemplars 8 6 8 4\\nRet. for exemplars ✓ ✗ ✗ ✗\\nTable 7: Dataset statistics and experimental settings of different tasks.\\nwikipedia.org, wikiwand.com, wiki2.org, wikimedia.org\\nTable 8: Wikipedia-related domains excluded from Bing’s search results.\\nDataset θ β Query formulation Combine single- & multi-time retrieval\\n2WikiMultihopQA 0.8 0.4 implicit ✗\\nStrategyQA 0.4 0.4 implicit ✗\\nASQA & ASQA-hint 0.8 0.4 explicit ✓\\nWikiAsp 0.8 0.4 explicit ✓\\nTable 9: Hyperparameters of FLARE on different datasets.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7ec7f16d-6284-42a6-8ad4-5f6704644028', embedding=None, metadata={'page_label': '17', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.3: retrieval instructions for 2WikiMultihopQA\\nSkill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you\\nwant to look up. For example:\\nQuestion: But what are the risks during production of nanomaterials?\\nAnswer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung\\ndamage.\\nQuestion: The colors on the flag of Ghana have the following meanings.\\nAnswer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for\\nmineral wealth.\\nQuestion: Metformin is the first-line drug for what?\\nAnswer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity.\\nSkill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclu-\\nsion. For example:\\nQuestion: When did the director of film Hypocrite (Film) die?\\nAnswer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So\\nthe answer is 19 June 2013.\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\nAnswer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of\\nRepublic of Macedonia. Thus, they are not in the same country. So the answer is no.\\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\\nnationality?\\nAnswer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by\\nKarl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\\nnationality. So the answer is no.\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\\nAnswer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\\nfather-in-law is Genghis Khan. So the answer is Genghis Khan.\\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\\nAnswer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\\nMartin Hodge was born first. So the answer is Martin Hodge.\\nQuestion: When did the director of film Laughter In Hell die?\\nAnswer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\\n1963. So the answer is August 25, 1963.\\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer (with step-by-step): The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took\\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nNow, combine the aforementioned two skills. First, write out the reasoning steps, then draw the conclusion,\\nwhere the reasoning steps should also utilize the Search API “[Search(term)]” whenever possible.\\nQuestion: Where did Minbyauk Thihapate’s wife die?\\nAnswer (with step-by-step & Search):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69467a1c-ef94-4706-bd7a-4d8e1bbcdd74', embedding=None, metadata={'page_label': '18', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.4: exemplars of 2WikiMultihopQA\\nQuestion: When did the director of film Hypocrite (Film) die?\\nAnswer: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19\\nJune 2013.\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\nAnswer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia.\\nThus, they are not in the same country. So the answer is no.\\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\\nnationality?\\nAnswer: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David\\nDhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the\\nanswer is no.\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\\nAnswer: Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is\\nGenghis Khan. So the answer is Genghis Khan.\\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\\nAnswer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was\\nborn first. So the answer is Martin Hodge.\\nQuestion: When did the director of film Laughter In Hell die?\\nAnswer: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\\nanswer is August 25, 1963.\\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\\nFrederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,\\nthe person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\\nhas a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nQuestion: Which country the director of film Citizen Mavzik is from?\\nAnswer:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4bfa3f0-ff9e-422b-83c2-194dd9c84d7c', embedding=None, metadata={'page_label': '19', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.5: exemplars of StrategyQA\\nGenerate a yes or no answer to the following question.\\nQuestion: Do hamsters provide food for any animals?\\nAnswer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the final\\nanswer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Could Brooke Shields succeed at University of Pennsylvania?\\nAnswer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the Uni-\\nversity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the final answer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\nAnswer: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number\\nsquared is less than 5. So the final answer is no.\\nGenerate a yes or no answer to the following question.\\nQuestion: Is it common to see frost during some college commencements?\\nAnswer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there\\ncan be frost. Thus, there could be frost at some commencements. So the final answer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\\nAnswer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months.\\nThus, a llama could not give birth twice during the War in Vietnam. So the final answer is no.\\nGenerate a yes or no answer to the following question.\\nQuestion: Would a pear sink in water?\\nAnswer: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a pear\\nwould float. So the final answer is no.\\nGenerate a yes or no answer to the following question.\\nQuestion: Did Spiderman fight against Falcon in the MCU?\\nAnswer:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f85e69a3-da4a-4d82-86df-5bc81d60b8bf', embedding=None, metadata={'page_label': '20', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.6: exemplars of ASQA\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: Who played bonnie in gone with the wind?\\nAnswer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In\\norder to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with\\nthe Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the Wind.\\nTherefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who played\\nBonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with the\\nWind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\ncharacter Bonnie was played by Leilah de Meza.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What is the second largest city in the usa?\\nAnswer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\nIn order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can be\\nmeasured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\narea is Juneau, Alaska.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When was bohemian rhapsody released as a single?\\nAnswer: This question is ambiguous in terms of which country’s release of the single is being referred to. In order to figure\\nout its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody\\nwas released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations:\\n(1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released\\nas a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the\\nUnited Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: Where do the philadelphia eagles play their home games?\\nAnswer: This question is ambiguous in terms of which specific location or venue is being referred to. In order to figure out its\\ninterpretations, we need to consider the different possible locations or venues that could be considered the home field of the\\nPhiladelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations:\\n(1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play\\ntheir home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations\\nare: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in\\nthe South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When did xbox one come out in australia?\\nAnswer: This question is ambiguous in terms of which specific version of the Xbox One is being referred to. In order to\\nfigure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One\\nhas two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2\\ninterpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The\\nanswers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013.\\n(2) The Xbox One X video game console was released in Australia on November 7, 2017.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When does the movie summer of 84 come out?\\nAnswer: This question is ambiguous in terms of which release of the movie is being referred to. In order to figure out its\\ninterpretations, we need to consider different releases of the movie Summer of ’84. The movie Summer of ’84 is first released\\nat the Sundance Festival before it’s released throughout the US. Therefore, this question has 2 interpretations: (1) When\\ndid the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie Summer of ’84 first release\\nthroughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the Sundance Festival on\\nJanuary 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89e87acc-59ab-4594-b114-b70a21adc9b6', embedding=None, metadata={'page_label': '21', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.7: exemplars of ASQA (cont.)\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What was roy orbison’s first number one hit?\\nAnswer: This question is ambiguous in terms of which specific chart or region is being referred to. In order to figure out its\\ninterpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\\nwas Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s first number one hit in\\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the first number one hit for Roy Orbison\\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the first number one hit for Roy Orbison in\\nthe UK and Ireland.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What is the criminal’s name in the breakfast club?\\nAnswer: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\\nname. In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\nThe Breakfast Club.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: How many state parks are there in virginia?\\nAnswer:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0ee8ad3-94fa-4c5d-8772-ac344805809b', embedding=None, metadata={'page_label': '22', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.8: exemplars of ASQA-hint\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\\nanswer them one by one.\\nQuestion: Who played bonnie in gone with the wind?\\nHint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.\\nGone with the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the\\nWind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who\\nplayed Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with\\nthe Wind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\ncharacter Bonnie was played by Leilah de Meza.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: What is the second largest city in the usa?\\nHint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\nAnswer: In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can\\nbe measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\narea is Juneau, Alaska.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: When was bohemian rhapsody released as a single?\\nHint: This question is ambiguous in terms of which country’s release of the single is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different countries where Bohemian Rhapsody is\\nreleased. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,\\nthis question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When\\nwas Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian\\nRhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a\\nsingle in the United States on December 1975.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: Where do the philadelphia eagles play their home games?\\nHint: This question is ambiguous in terms of which specific location or venue is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different possible locations or venues that could be\\nconsidered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore,\\nthis question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports\\ncomplex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home\\ngames? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2)\\nPhiladelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their\\nhome games in the Lincoln Financial Field stadium.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: When did xbox one come out in australia?\\nHint: This question is ambiguous in terms of which specific version of the Xbox One is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different versions of the Xbox One that have been\\nreleased. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this\\nquestion has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release\\nin Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on\\nNovember 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: When does the movie summer of 84 come out?\\nHint: This question is ambiguous in terms of which release of the movie is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different releases of the movie Summer of ’84. The\\nmovie Summer of ’84 is first released at the Sundance Festival before it’s released throughout the US. Therefore, this question\\nhas 2 interpretations: (1) When did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie\\nSummer of ’84 first release throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the\\nSundance Festival on January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc99f434-8efc-4556-bd3e-842846a17852', embedding=None, metadata={'page_label': '23', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.9: exemplars of ASQA-hint (cont.)\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\\nanswer them one by one.\\nQuestion: What was roy orbison’s first number one hit?\\nHint: This question is ambiguous in terms of which specific chart or region is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different charts and regions where Roy Orbison’s\\nmusic was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\\nquestion has 2 interpretations: (1) What was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What\\nwas Roy Orbison’s first number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\\nwas the first number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\\nthe first number one hit for Roy Orbison in the UK and Ireland.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: What is the criminal’s name in the breakfast club?\\nHint: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\\nname.\\nAnswer: In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\nThe Breakfast Club.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: How many state parks are there in virginia?\\nHint: This question is ambiguous in terms of the time frame or period being referred to.\\nAnswer:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1788fa34-5a20-40bc-a858-b5b25270583d', embedding=None, metadata={'page_label': '24', 'file_name': 'Active Retrieval Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Active Retrieval Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 774851, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt D.10: exemplars of WikiAsp\\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\\n# Location\\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\\nat 39°56’12\"N 32°51’55\"E.\\n# History\\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸ Serafettin after whom the mosque was named.\\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\\nGenerate a summary about Untold Legends: The Warrior’s Code including the following aspects: reception,\\ngameplay, development with one aspect per line.\\n# Reception\\nThe game received \"mixed or average reviews\" according to video game review aggregator Metacritic.\\n# Gameplay\\nThe warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.\\n# Development\\nAs a pre-order bonus, the game was shipped with a small action figure of the Guardian class.\\nGenerate a summary about Raid on St. Augustine including the following aspects: aftermath, background with\\none aspect per line.\\n# Aftermath\\nOnce the English had gone Menéndez and the rest of the Spanish settlers returned to find a smoldering ruins and very little\\nleft. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\\ndestroyed fort was replaced with the present day Castillo de San Marcos.\\n# Background\\nWar had already been unofficially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\\noffered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\\nEngland, he struck first at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\\nDomingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\\ncity of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before finally visiting and\\nreplenishing Sir Walter Raleigh’s new colony of Roanoke Colony on the American East Coast. Then after this he hoped\\nto make the Transatlantic crossing back to England. The fleet headed north, and in late April Drake put into the Spanish\\nCuban mainland and his men dug wells in search of fresh water and gathered supplies to help counter an outbreak of\\ndysentery after which he moved on. The fleet traveled north within sight of land on the Florida peninsula sailing past\\nthe West coast. On 27 May 1586 as they approached further north a small fort was spotted on the shore, with a small\\ninlet close by. This was the location of St Augustine, the most northerly town in Spain’s New World Empire, and the\\noldest permanent colonial settlement in North America. Drake knew of the place and was also aware of the fact that\\nthe spanish under Pedro Menéndez de Avilés had ordered all of the French Huguenot colonists that had tried to settle\\nin the area executed. Drake decided on one final opportunity to raid and plunder, and a chance to avenge his fellow Protestants.\\nGenerate a summary about Lakewood (Livingston, Alabama) including the following aspects: architecture, his-\\ntory with one aspect per line.\\n# Architecture\\nThe house has a plan that is relatively rare in early Alabama architecture. The plan features a brick ground floor that is topped\\nby one-and-a-half-stories of wood-frame construction. The ground floor originally contained domestic spaces, with the\\nformal rooms on the principle floor and bedrooms on the upper floor. A central hallway is present on all levels. The facade is\\nfive bays wide, with central entrance doors on the ground and principle floors. The bays are divided by two-story Doric\\npilasters, with the middle third of the facade occupied by a two-tiered tetrastyle Doric portico. Two curved wrought iron\\nstaircases ascend from ground level to the front center of the upper portico, leading to the formal entrance.\\n# History\\nLakewood was built for Joseph lake, a native of North Carolina, by Hiram W. Bardwell, a master builder. Construction\\nwas completed in 1840. Located adjacent to the University of West Alabama, Julia Strudwick Tutwiler, a Lake relative,\\nperiodically resided in the house from 1881 to 1910 while she served as president of the university. It was then known as\\nLivingston Normal College. The house was extensively photographed by Alex Bush for the Historic American Buildings\\nSurvey in November and December 1936. Lakewood has continued to be owned by descendants of the Lake family to the\\ncurrent day. The house and its surviving 10 acres (4.0 ha) of grounds were listed on the Places in Peril in 2012 due to the\\nimmediate threat of its acquisition by developers.\\nGenerate a summary about Carlos Moedas including the following aspects: biography, early life, political career\\nwith one aspect per line.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9bd71665-45b7-42e7-8424-b95c1ab015f9', embedding=None, metadata={'page_label': '1', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b90be63c-23b2-4ee5-90f9-6a70b6de5ffc', embedding=None, metadata={'page_label': '2', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional\\ncomputation [ 26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,16]. In all but a few cases [ 22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='352befc1-db77-44ad-9ac0-1d6a591e5f51', embedding=None, metadata={'page_label': '3', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [ 10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4389e9b-67fd-41ab-bbb2-d7267ba5948e', embedding=None, metadata={'page_label': '4', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=∑dk\\ni=1qiki, has mean 0and variance dk.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76128841-8b16-4ee9-b57b-e741e44a1865', embedding=None, metadata={'page_label': '5', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 24]. In the embedding layers, we multiply those weights by√dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8b031af-2f88-4e65-8e76-bff6a2e0c77c', embedding=None, metadata={'page_label': '6', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a841716f-cd0c-4f77-9b23-2f33630757a8', embedding=None, metadata={'page_label': '7', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 17] withβ1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5,step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85e2aa81-967e-4e76-bda1-a944bbba95aa', embedding=None, metadata={'page_label': '8', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0·1020\\nGNMT + RL [31] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [8] 25.16 40.46 9.6·10181.5·1020\\nMoE [26] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.0 2.3·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea872220-0ff9-4e19-a4a4-2423e5fa4ed2', embedding=None, metadata={'page_label': '9', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35292291-4d7f-4814-a802-0375614d0cd5', embedding=None, metadata={'page_label': '10', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f55cdb6f-e738-43d9-97bb-1ba3a1964714', embedding=None, metadata={'page_label': '11', 'file_name': 'Attention is All you Need.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Attention is All you Need.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-03-30', 'last_modified_date': '2024-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9caee674-0859-444e-b9d2-95ed0242c205', embedding=None, metadata={'page_label': '1', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Benchmarking Large Language Models in Retrieval-Augmented Generation\\nJiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2\\n1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China\\n2State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China\\n3University of Chinese Academy of Sciences, Beijing, China\\n{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn\\nAbstract\\nRetrieval-Augmented Generation (RAG) is a promising ap-\\nproach for mitigating the hallucination of large language\\nmodels (LLMs). However, existing research lacks rigorous\\nevaluation of the impact of retrieval-augmented generation\\non different large language models, which make it challeng-\\ning to identify the potential bottlenecks in the capabilities\\nof RAG for different LLMs. In this paper, we systemati-\\ncally investigate the impact of Retrieval-Augmented Gener-\\nation on large language models. We analyze the performance\\nof different large language models in 4 fundamental abili-\\nties required for RAG, including noise robustness, negative\\nrejection, information integration, and counterfactual robust-\\nness. To this end, we establish Retrieval-Augmented Genera-\\ntion Benchmark (RGB), a new corpus for RAG evaluation in\\nboth English and Chinese. RGB divides the instances within\\nthe benchmark into 4 separate testbeds based on the afore-\\nmentioned fundamental abilities required to resolve the case.\\nThen we evaluate 6 representative LLMs on RGB to diag-\\nnose the challenges of current LLMs when applying RAG.\\nEvaluation reveals that while LLMs exhibit a certain degree\\nof noise robustness, they still struggle significantly in terms of\\nnegative rejection, information integration, and dealing with\\nfalse information. The aforementioned assessment outcomes\\nindicate that there is still a considerable journey ahead to ef-\\nfectively apply RAG to LLMs.\\nIntroduction\\nRecently, there have been impressive advancements in large\\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\\nChatGLM (THUDM 2023a). Although these models have\\nshown remarkable general abilities (Bang et al. 2023; Guo\\net al. 2023), they still suffer severely from challenges includ-\\ning factual hallucination (Cao et al. 2020; Raunak, Menezes,\\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-\\ndating (He, Zhang, and Roth 2022), and the lack of domain-\\nspecific expertise (Li et al. 2023c; Shen et al. 2023).\\nIncorporating external knowledge via information re-\\ntrieval, i.e., Retrieval-Augmented Generation (RAG), has\\nbeen regarded as a promising way to resolve the above chal-\\nlenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\\n*Corresponding authors.\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\nNoise Robustness Negative Rejection\\nWho was awarded the 2022 \\nNobel prize in literature?\\n… French author Annie Ernaux\\n… in Literature for 2021 is…\\nAnnie ErnauxQuestion\\nExternal documents contain noises\\nRetrieval Augmented \\nGenerationThe 2020 Nobel Laureate …\\nI can not answer the question …\\nInformation Integration\\nWhen were the ChatGPT app for \\niOS and api launched?\\nOn May 18th, 2023, OpenAI…\\nThat changed on March 1, …\\nMay 18 and March 1.Question\\nExternal documents contain all answers\\nRetrieval Augmented \\nGenerationCounterfactual Robustness\\nWhich city hosted the Olympic \\ngames in 2004?\\n2004 Olympic …to New York , \\nNew York  easily defeated …\\nThere are factual errors in the…Question\\nCounterfactual external documents\\nRetrieval Augmented \\nGenerationRetrieval Augmented \\nGenerationWho was awarded the 2022 \\nNobel prize in literature?Question\\n… in Literature for 2021 is…External documents are all noisesFigure 1: Illustration of 4 kinds of abilities required for\\nretrieval-augmented generation of LLMs.\\n2022; Izacard et al. 2022). With the help of external knowl-\\nedge, LLMs can generate more accurate and reliable re-\\nsponses. The most common method is to use a search engine\\nas a retriever such as New Bing. Due to the vast amount of\\ninformation available on the Internet, using a search engine\\ncan provide more real-time information.\\nHowever, Retrieval-Augmented Generation brings not\\nonly positive effects to LLMs (Liu, Zhang, and Liang 2023;\\nMaynez et al. 2020). On one hand, there is a significant\\namount of noise information even fake news in the content\\navailable on the Internet, which poses challenges for search\\nengines in accurately retrieving desirable knowledge. On the\\nother hand, LLMs suffer from unreliable generation chal-\\nlenge. LLMs can be misled by incorrect information con-\\ntained in the context (Bian et al. 2023) and also suffer from\\nhallucination during the generation (Adlakha et al. 2023),\\nresulting in generating content that goes beyond external in-\\nformation. These challenges result in LLMs being unable to\\nconsistently generate reliable and accurate responses. Un-\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17754', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96aca4b7-f9ea-4365-b4d7-4875602cf259', embedding=None, metadata={'page_label': '2', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='fortunately, currently there lacks of comprehensive under-\\nstanding on how these factors can influence RAG, and how\\ncould each model survives from these drawbacks and im-\\nprovement their performance via information retrieval. As a\\nresult, there is a pressing need for a comprehensive evalua-\\ntion of LLMs on their ability to effectively utilize retrieved\\ninformation, as well as their ability to withstand the various\\ndrawbacks present in information retrieval.\\nTo this end, this paper conducts a comprehensive evalua-\\ntion of RAG for current LLMs. Specifically, we create a new\\nRetrieval-Augmented Generation Benchmark, namely RGB,\\nin both English and Chinese. In order to ensure that the in-\\nternal knowledge of LLMs does not introduce bias into the\\nevaluation results, RGB chooses to aggregate the latest news\\ninformation and constructs queries based on the news infor-\\nmation. Then, based on these queries, we use Search API to\\nfetch relevant documents and select most relevant snippets\\nfrom the content as external retrieved documents. Finally,\\nbased on different compositions of query and document-set\\npairs, we expand the corpus and divided it into 4 testbeds to\\nevaluate the following basic abilities of LLMs according to\\nthe common challenges in RAG, as shown in Figure 1:\\n•Noise Robustness, which means a LLM can extract use-\\nful information from noisy documents. In this paper, we\\ndefine noisy documents as those that are relevant to the\\nquestion but do not contain any information of the an-\\nswer. For the instance in Figure 1, the noisy documents\\nrelated to the question “Who was awarded the 2022 No-\\nbel Prize in Literature” include reports about the 2021\\nNobel Prize in Literature. To this end, the testbed for\\nnoise robustness contains instances whose external doc-\\numents contain a certain number of noisy documents\\nbased on the desired noise ratio.\\n•Negative Rejection, which means that a LLM should re-\\nject to answer the question when the required knowledge\\nis not present in any retrieved document. The testbed for\\nnegative rejection contains instances whose external doc-\\numents are only with noisy documents. LLMs are ex-\\npected to indicate “insufficient information” or other re-\\njection signals.\\n•Information Integration, which evaluates whether\\nLLMs can answer complex questions that require inte-\\ngrating information from multiple documents. For the in-\\nstance in Figure 1, for the question “When were the Chat-\\nGPT app for iOS and ChatGPT api launched?”, LLMs\\nare expected to provide information of the launch dates\\nfor both the ChatGPT iOS app and ChatGPT API. The\\ntestbed for information integration contains instances\\nthat can only be answered using multiple documents.\\n•Counterfactual Robustness, which evaluates whether\\nLLMs can identify risks of known factual errors in the\\nretrieved documents when the LLMs are given warnings\\nabout potential risks in the retrieved information through\\ninstruction. The testbed for counterfactual robustness in-\\ncludes instances that can be answered directly by the\\nLLMs, but the external documents contain factual errors.\\nBased on RGB, we conduct evaluation on 6 state-of-\\nthe-art large language models including ChatGPT (Ope-nAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\\n6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\\nQwen-7B-Chat (Bai et al. 2023), BELLE-7B (BELLEGroup\\n2023). We found that even though RAG can improve the re-\\nsponse accuracy of LLMs, they still suffer from the above-\\nmentioned challenges significantly. Specifically, we found\\nthat even though LLMs demonstrate some level of noise ro-\\nbustness, they tend to confuse similar information and fre-\\nquently generate inaccurate answers when relevant informa-\\ntion exists. For example, when faced with a question about\\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\\nments about the 2021 Nobel Prize in Literature in external\\ndocuments, LLMs may become confused and provide inac-\\ncurate answers. Besides, LLMs frequently fail to reject an-\\nswering and generate incorrect answers when none of the\\nexternal documents contain relevant information. Further-\\nmore, LLMs lack the ability to summarize from multiple\\ndocuments, and therefore if multiple documents are needed\\nto answer a question, LLMs often fail to provide accurate\\nanswer. Finally, we found that even when the LLMs contain\\nthe required knowledge and are given warnings about po-\\ntential risks in the retrieved information through instruction,\\nthey still tend to trust and prioritize the retrieved information\\nover their own existing knowledge. The experimental results\\nmentioned above highlight the need for further resolution of\\nimportant issues in the existing RAG method. Therefore, it\\nis crucial to exercise caution and carefully design its usage.\\nGenerally speaking, the contributions of this paper are1:\\n• We proposed to evaluate four capabilities for retrieval-\\naugmented generation of LLMs and created the\\nRetrieval-Augmented Generation Benchmark in both En-\\nglish and Chinese. To best of our knowledge, it is the first\\nbenchmark designed to assess these four capabilities for\\nretrieval-augmented generation of LLMs.\\n• We evaluated the existing LLMs using RGB and found\\nthe limitations of them in the four different abilities.\\n• We analyzed the responses of LLMs in RGB and identi-\\nfied their current shortcomings as well as suggested di-\\nrections for improvement.\\nRelated Work\\nRetrieval-augmented models The knowledge stored in\\nlarge language models is commonly out-of-date (He, Zhang,\\nand Roth 2022) and they also sometimes generate hallu-\\ncination (Cao et al. 2020; Raunak, Menezes, and Junczys-\\nDowmunt 2021; Ji et al. 2023) i.e., they may generate ir-\\nrelevant or factually incorrect contents. By using external\\nknowledge as guidance, retrieval-augmented models can\\ngenerate more accurate and reliable responses (Guu et al.\\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-\\naugmented models have achieved remarkable results in var-\\nious tasks such as open-domain QA (Izacard and Grave\\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\\net al. 2019a,b; Peng et al. 2023), domain-specific ques-\\ntion answering (Cui et al. 2023) and code generation (Zhou\\n1Our code&data: https://github.com/chen700564/RGB.\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17755', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='823f2a2a-dc4e-4101-956c-b0bb7595457b', embedding=None, metadata={'page_label': '3', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='News CollectionNews about The 2022 Nobel Prize for Physiology and MedicineData adjustment and filtering by Human{   “Question”: “Who was awarded the 2022 Nobel Prize for Physiology and Medicine?”,   “Answer”: [\\'Svante Pääbo\\',\\'Svante Paabo’]}Data generation by ChatGPTRetrieve using search engineRerank by dense retrieval model\\nWe simulate the process of a user querying and obtaining information.. ……News: The 2022 Nobel Prize for …Related event: … \\\\nQuestion:… \\\\nKey information:…gpt-3.5-turbo apiQuery: Who was awarded the 2022 Nobel Prize for Physiology and Medicine?”,{\"link\": \"https://www.nobelprize.org/prizes/medicine/\", ...Google Search APIChun2ChunkQueryDense retrieval modelTop1 ChunkTop30 ChunkTop2 Chunk…………Figure 2: The process of data generation. Firstly, we use\\nmodels to extract (event, question, answer) from news ar-\\nticles. Next, we utilize search engines to retrieve relevant\\nweb pages. Finally, a dense retrieval model is employed to\\nre-rank the content of these web pages.\\net al. 2023b). Recently, with the development of large mod-\\nels, a series of retrieval-enhanced tools and products have\\ngained widespread attention, such as ChatGPT retrieval plu-\\ngin, Langchain, New Bing, etc. However, in real-world sce-\\nnarios, the retrieved text inevitably contains noise. There-\\nfore, in this paper we conducted a systematic evaluation and\\nanalysis of retrieval-augmented generation in LLMs.\\nEvaluation of LLMs Evaluating LLMs has received sig-\\nnificant attention due to their remarkable general capabil-\\nity (Chang et al. 2023). It enables us to gain a deeper under-\\nstanding of the specific abilities and limitations of LLMs,\\nwhile also providing valuable guidance for future research.\\nIn the past, benchmarks such as GLUE (Wang et al. 2019b)\\nand SuperCLUE (Wang et al. 2019a) primarily focused on\\nevaluating NLP tasks, particularly in natural language un-\\nderstanding. However, these evaluations often fail to fully\\ncapture the capabilities of LLMs. MMLU (Hendrycks et al.\\n2021) was then proposed to measure the knowledge acquired\\nby language models when pre-training. Recently, with the\\ndevelopment of LLMs, a series of general evaluation bench-\\nmarks have emerged, such as AGIEval (Zhong et al. 2023),\\nC-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),\\nOpenLLM Leaderboard (Edward Beeching 2023), etc. In\\naddition to general abilities, there are also specific bench-\\nmarks that focus on evaluating the capabilities of models.\\nFor example, CValues (Xu et al. 2023a) focuses on the safetyand responsibility of LLMs, M3Exam (Zhang et al. 2023)\\nfocuses on human exam and ToolBench (Qin et al. 2023)\\nevaluates how well LLMs use external tools. Recently, Ad-\\nlakha et al. (2023) evaluate the RAG of LLMs in exist QA\\ndataset. Different from their work, we focus on 4 required\\nabilities of RAG and create Retrieval-Augmented Genera-\\ntion Benchmark to evaluate the LLMs.\\nRetrieval-Augmented Generation Benchmark\\nIn this section, we first introduce the specific retrieval-\\naugmented generation abilities we aim to evaluate. Next, we\\noutline the process of constructing the RAG benchmark for\\nevaluation. Lastly, we present the evaluation metrics.\\nRequired Abilities of RAG\\nExternal knowledge is the key to resolving the problems\\nof LLMs such as hallucination and outdated knowledge,\\nwhich can make LLMs generate more accurate and reliable\\nresponses through retrieval-augmented generation (RAG).\\nHowever, LLMs cannot always response as expected with\\nRAG. For one thing, there are numerous irrelevant docu-\\nments and false information on the Internet. Incorporating\\nthese external documents into LLMs could have a detrimen-\\ntal effect. For anthoer, LLMs suffer from the unreliable gen-\\neration challenge. The generation of LLMs is often unpre-\\ndictable, and we cannot guarantee that they will utilize the\\nuseful information entailed in the external documents. Ad-\\nditionally, LLMs can easily be misled by incorrect infor-\\nmation in the document. To this end, we build Retrieval-\\nAugmented Generation Benchmark (RGB) to evaluate the\\nretrieval-augmented generation of LLMs, and we concern\\nabout 4 specific abilities:\\nNoise Robustness is the robustness of LLMs in noisy\\ndocuments. As retrievers are not perfect, the external knowl-\\nedge they retrieve often contains a significant amount of\\nnoise, i.e., documents which are relevant to the question but\\ndo not contain any information about the answer. To effec-\\ntively answer user questions, LLMs must be able to extract\\nthe necessary information from documents despite there are\\nnoisy documents.\\nNegative Rejection is a measure of whether LLMs can\\ndecline to answer a question when none of the contexts pro-\\nvide useful information. In real-world situations, the search\\nengine often fails to retrieve documents containing the an-\\nswers. In these cases, it is important for the model to have\\nthe capability to reject recognition and avoid generating mis-\\nleading content.\\nInformation Integration is a capacity to integrate an-\\nswers from multiple documents. In many cases, the an-\\nswer to a question may be contained in multiple documents.\\nFor example, for the question “Who are the champions of\\nthe U.S. Open 2022 men’s and women’s singles?” , the two\\nchampions may be mentioned in different documents. In or-\\nder to provide better answers to complex questions, it is nec-\\nessary for LLMs to have the ability to integrate information.\\nCounterfactual Robustness refers to a capacity to han-\\ndle errors in external knowledge. In the real world, there is\\nan abundance of false information on the internet. Please\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI- 24)\\n17756', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='946964ee-0df7-4576-bff3-8ec33ed62617', embedding=None, metadata={'page_label': '4', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='note that we only evaluate the situation that LLMs are given\\nwarnings about potential risks in the retrieved information\\nthrough instruction.\\nIn real-world scenarios, it is not possible to obtain per-\\nfect documents with all the necessary external knowledge.\\nTherefore, evaluating these four abilities of the model be-\\ncomes essential in order to measure the RAG of LLMs.\\nData Construction\\nInspired by previous benchmarks for LLMs, RGB utilizes\\na question-answering format for evaluation. We evaluate the\\nLLMs by judging the retrieval-augmented responses of them\\nto the questions. To simulate real-world scenarios, we con-\\nstruct question and answer data using actual news articles.\\nDue to the abundance of knowledge contained within the\\nLLMs there is a potential for bias when measuring the first\\nthree abilities. To mitigate this, the instances of RGB are\\nconstructed by latest news articles. Additionally, we retrieve\\nexternal documents from Internet through search engines.\\nFinally, we expand the corpus and divided it into 4 testbeds\\nto evaluate the above basic abilities of LLMs. The overall\\nprocedure of our data construction is illustrated in Figure 2.\\nQA instances generation. We first collect latest news ar-\\nticles and use prompts to make ChatGPT generate events,\\nquestions, and answers for each articles. For example, as\\nshown in the Figure 2, for a report about “The 2022 Nobel\\nPrize”, ChatGPT will generate corresponding event, ques-\\ntion and provide key information for answering it. By gen-\\nerating events, the model is able to preliminarily filter out\\nnews articles that do not contain any events. After genera-\\ntion, we manually check the answer and filter out data that\\nis difficult to retrieve through search engines.\\nRetrieve using search engine. For each query, we use\\nGoogle’s API to fetch 10 relevant web pages and extract cor-\\nresponding snippets of text from them. Simultaneously, we\\nread these web pages and convert their textual content into\\ntext chunks with a maximum length of 300 tokens. Using an\\nopen-source dense retriever, we select the top-30 text chunks\\nthat match the query most effectively. These retrieved text\\nchunks, along with the snippets provided by the search API,\\nwill serve as our external documents. These documents will\\nbe divided into positive documents and negative documents\\nbased on whether they contain the answer.\\nTestbeds construction for each ability. We expand the\\ncorpus and divided it into 4 testbeds to evaluate the above\\nbasic abilities of LLMs. To evaluate the noise robustness,\\nwe sample varying numbers of negative documents ac-\\ncording to the desired ratio of noises. For negative rejec-\\ntion, all the external documents are sampled from negative\\ndocuments. For the information integration ability, we fur-\\nther construct data based on the above generated questions.\\nThis involves expanding or rewriting these questions so that\\ntheir answers encompass multiple aspects. For example, the\\nquestion “Who won the MVP of Super Bowl 2023?” can\\nbe rewrite as “Who won the MVPs of Super Bowl 2022\\nand 2023?”. Consequently, answering such questions re-\\nquires utilizing information from various documents. Dif-\\nferent from the first three abilities, the data of counterfactual\\nrobustness is constructed solely based on the internal knowl-edge of the model. Based on the aforementioned generated\\nquestions mentioned above, we adopt ChatGPT to automat-\\nically generate its known knowledge. Specifically, we use\\nprompts to allow the model to generate both questions and\\nanswers that are already known. For example, based on the\\nquestion “Who was awarded the 2022 Nobel Prize for Phys-\\niology and Medicine?”, the model will generate the known\\nquestion “Who was awarded the 2021 Nobel Prize in Lit-\\nerature?” and answer “Abdulrazak Gurnah”. We then man-\\nually verified the generated answers, and retrieve relevant\\ndocuments as described above. In order to make documents\\ncontain factual errors, we manually modify the answers and\\nreplace the corresponding parts in the document.\\nFinally, we collect totally 600 base questions in RGB,\\nand 200 additional questions for the information integration\\nability and 200 additional questions for counterfactual ro-\\nbustness ability. Half of the instances are in English, and the\\nother half are in Chinese.\\nEvaluation Metrics\\nThe core of this benchmark is to evaluate whether LLMs can\\nutilize the provided external documents to acquire knowl-\\nedge and generate reasonable answers. We evaluate the re-\\nsponses of LLMs in order to measure above-mentioned four\\nabilities of them.\\nAccuracy is used to measure noise robustness and infor-\\nmation integration. We employ an exact matching approach\\nwhere if the generated text contains an exact match to the\\nanswer, it is considered as a correct answer.\\nRejection rate is used to measure negative rejection.\\nWhen only noisy documents are provided, LLMs should\\noutput the specific content – “I can not answer the question\\nbecause of the insufficient information in documents.” (We\\nuse instructions to inform the model.). If the model gener-\\nates this content, it indicates a successful rejection.\\nError detection rate measures whether the model can\\ndetect the factual errors in the documents for counterfactual\\nrobustness. When the provided documents contain factual\\nerrors, the model should output the specific content – “There\\nare factual errors in the provided documents.” (We use in-\\nstructions to inform the model.). If the model generates this\\ncontent, it indicates that the model has detected erroneous\\ninformation in the document.\\nError correction rate measures whether the model can\\nprovide the correct answer after identifying errors for coun-\\nterfactual robustness. The model is asked to generate the cor-\\nrect answer after identifying the factual errors. If the model\\ngenerates the correct answer, it indicates that the model is\\ncapable of correcting errors in the document.\\nConsidering that LLMs may not fully adhere to instruc-\\ntions, for rejection rate and error detection rate, we also use\\nChatGPT to conduct additional evaluation of the responses.\\nSpecifically, we prompt ChatGPT to determine if the re-\\nsponses can reflect information that is not present in the doc-\\nument or identify any factual errors.\\nExperiments\\nIn this section, we evaluate the performance of various\\nLLMs, analyze and discuss the results, summarizing the\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17757', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10d4cc45-9744-4f57-a12d-aebe55872314', embedding=None, metadata={'page_label': '5', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='English Chinese\\nNoise Ratio 0 0.2\\n0.4 0.6 0.8 0 0.2\\n0.4 0.6 0.8\\nChatGPT (OpenAI\\n2022) 96.33 94.67\\n94.00 90.00 76.00 95.67 94.67\\n91.00 87.67 70.67\\nChatGLM-6B (THUDM 2023a) 93.67 90.67\\n89.33 84.67 70.67 94.33 90.67\\n89.00 82.33 69.00\\nChatGLM2-6B (THUDM 2023b) 91.33 89.67\\n83.00 77.33 57.33 86.67 82.33\\n76.67 72.33 54.00\\nVicuna-7B-v1.3 (Chiang et al. 2023) 87.67 83.33\\n86.00 82.33 60.33 85.67 82.67\\n77.00 69.33 49.67\\nQwen-7B-Chat (Bai et al. 2023) 94.33 91.67\\n91.00 87.67 73.67 94.00 92.33\\n88.00 84.33 68.67\\nBELLE-7B-2M (BELLEGroup 2023) 83.33 81.00\\n79.00 71.33 64.67 92.00 88.67\\n85.33 78.33 67.68\\nTable 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\\nincreasing noise rate poses a challenge for RAG in LLMs.\\nLong-distance inf\\normation. Evidence uncertainty\\n. Concept confusion.\\nQuestionWho did\\nIga Swiatek defeat to\\nwin the Qatar Open 2022?What is\\nthe name of Apple’s headset?What w\\nas Tesla’s revenue in Q1\\n2022?\\nAnswer Anett K\\nontaveit Vision\\nPro 18.76 billion\\nDocsPositive\\ndocument\\nSwiatek entered into the Qatar\\nOpen ...won ... Anett Kontaveit\\nNegative document\\n... she defeated Ons Jabeur 6-2,\\n7-6(5) to win the 2022 US OpenPositive\\ndocument\\nApple unveiled a costly augmented-\\nreality headset called the Vision Pro\\nNegative document\\n... is what Gurman believes will be\\ncalled Apple Reality Pro. ...Positive\\ndocument\\nTesla, Inc. reported Q1 FY 2022\\n... revenues of $18.76 billion\\nNegative document\\n...earnings for 2022 ...Automotive\\nrevenue reached $16.86 billion\\nResponses Iga\\nSwiatek defeated Ons Jabeur ...headset\\nisApple Reality Pro. ...in\\nQ1 2022 was $16.86 billion.\\nTable 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses\\nare generated by ChatGLM2-6B. The bold text indicates the matching parts between the document and the question or answer,\\nwhile the italicized text highlights the non-matching parts.\\nmain challenges that existing LLMs encounter when using\\nexternal knowledge.\\nSettings\\nTask formats. We provide 5 external documents for each\\nquestion. In our experiments on noise robustness, we evalu-\\nate scenarios with noise ratios ranging from 0 to 0.8.\\nModels We evaluate 6 state-of-the-art LLMs includ-\\ning ChatGPT (OpenAI 2022) (gpt-3.5-turbo), ChatGLM-\\n6B (THUDM 2023a), ChatGLM2-6B (THUDM 2023b),\\nVicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-Chat (Bai\\net al. 2023), BELLE-7B-2M (BELLEGroup 2023).\\nResults on Noise Robustness\\nWe evaluated the accuracy based on the different noise ratios\\nin external documents, and the results are shown in Table 1.\\nWe can see that:\\n(1) RAG can effect improve the responses of LLMs.\\nLLMs have shown strong performance even in the presence\\nof noise, indicating that RAG is a promising way for LLMs\\nto generate accurate and reliable responses.\\n(2) The increasing noise rate poses a challenge for\\nRAG in LLMs. Specifically, when the noise ratio exceeds\\n80%, the accuracy decreases significantly at a significance\\nlevel of 0.05. For example, the performance of ChatGPT hasdecreased from 96.33% to 76.00%, while the performance\\nof ChatGLM2-6B has decreased from 91.33% to 57.33%.\\nError Analysis To better comprehend the negative impact\\nof noise on model generation, we examined the incorrect an-\\nswers and found that these errors typically originate from\\nthree reasons, as shown in Table 2.\\n(1) Long-distance information. LLMs often face diffi-\\nculty in identifying the correct answer from external docu-\\nments when the information related to the question is distant\\nfrom the information related to the answer. This scenario\\nis quite common as longer texts are frequently encountered\\non the internet. In such cases, it is typical for the question’s\\ninformation to be initially presented at the start of the doc-\\nument and subsequently referred to using pronouns. In Ta-\\nble 2, the question information (“Qatar Open 2022”) is only\\nmentioned once at the beginning and is far from where the\\nanswer text “Anett Kontaveit” appears. This situation may\\ncause LLMs to depend on information from other docu-\\nments and create false impressions, i.e., hallucination.\\n(2) Evidence uncertainty. Before highly anticipated\\nevents, like the release of new Apple products or the an-\\nnouncement of the Oscars, there is often a significant\\namount of speculative information circulating on the inter-\\nnet. Although the relevant documents explicitly state that\\nit is uncertain or speculative content, they can still impact\\non the retrieval-augmented generation of LLMs. In Table 2,\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17758', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10e82110-96bc-4307-b3b8-ab73d27ce907', embedding=None, metadata={'page_label': '6', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Languages English Chinese\\nRej Rej∗Rej Rej∗\\nChatGPT 24.67 45.00 5.33 43.33\\nChatGLM-6B 9.00 25.00 6.33 17.00\\nChatGLM2-6B 10.33 41.33 6.33 36.33\\nV\\nicuna-7B-v1.3 17.00 33.33 3.37 24.67\\nQwen-7B-Chat 31.00 35.67 8.67 25.33\\nBELLE-7B-2M 5.67 32.33 5.33 13.67\\nTable 3: The result of negative rejection. Rej means the re-\\njection rate (%) and Rej∗means the rejection rate evaluated\\nby ChatGPT. We can see that negative rejection poses a chal-\\nlenge for RAG in LLMs.\\nwhen the noise ratio increases, the content of erroneous\\ndocuments is all about some people’s predictions about the\\nname of the headset (“Apple Reality Pro”). Even if there is\\na correct answer (“Vision Pro”) in the relevant documents,\\nLLMs can still be misled by uncertain evidences.\\n(3) Concept confusion. The concepts in external docu-\\nments may be similar to, but different from, the concepts in\\nthe question. This can cause confusion for LLMs and make\\nLLMs generate incorrect answers. In Table 2, the model an-\\nswer focuses on the concept “automotive revenue” in the\\ndocument rather than “revenue” in the question.\\nBased on the analysis above, we have identified certain\\nlimitations in LLMs regarding retrieval-augmented genera-\\ntion. To effectively handle the vast amount of noise present\\non the internet, further detailed enhancements are required\\nfor the model such as long documents modeling and precise\\nconcept comprehension.\\nResults on Negative Rejection Testbed\\nWe evaluated the rejection rate when only noise documents\\nwere provided. The results are shown in Table 3. In addi-\\ntion to evaluating the rejection rate through exact matching\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\nthe responses from the LLMs contain any rejection informa-\\ntion (Rej∗in Table 3). We can see that: Negative Rejection\\nposes a challenge for RAG in LLMs. The highest rejection\\nrates for LLMs in English and Chinese were only 45% and\\n43.33%, respectively. This suggests that LLMs can be easily\\nmisled by noisy documents, leading to incorrect answers.\\nIn addition, through comparing Rej and Rej∗, we found\\nthat LLMs fail to strictly follow instructions, and they often\\ngenerate unpredictable responses, which make it hard to use\\nthem as state triggers (such as for recognizing rejection).\\nWe conduct case studies in Table 4. The first error is\\nbecause of Evidence uncertainty. Although the document\\nonly mentions contact with “Adam McKay” and does not\\nexplicitly state that he is the director of the movie, the\\nmodel still concludes that he holds this role. The first er-\\nror is because of Concept confusion. The information pro-\\nvided in the answer pertains to “the 2018 Winter Olympics”\\ninstead of “the 2022 Olympics” mentioned in the question.\\nRetrieval-augmented generation poses a greater challenge ofQuestion Answer Response\\nwho will\\ndirect\\nIrredeemable film?Jeymes\\nSamuel...Adam McKay to\\nmo\\nvie adaptation of\\n“Irredeemable” from\\nWhich country\\nw-\\non the most medals\\nat the 2022 Winter\\nOlympics?Norway... that\\nwon the most\\nmedals ... is German-\\ny. It has won a total\\nof 31 medals ...\\nTable 4: Error cases of negative rejection generated by\\nChatGLM2-6B. The bold text highlights the error answers.\\nnegative rejection compared to answer directly as it presents\\nrelevant documents that could potentially mislead the LLMs\\nand result in incorrect responses. In future developments, it\\nwill be crucial for LLMs to enhance their ability to accu-\\nrately match questions with the appropriate documents.\\nResults on Information Integration Testbed\\nWe evaluated the accuracy based on the different noise ratios\\nin external documents, and the results are shown in Table 5.\\nWhen comparing the model to Table 1, we observed that\\nit has a weak information integration ability, which in turn\\naffects its noise robustness. We can see that:\\n(1) Information integration poses a challenge for RAG\\nin LLMs. Even without noise, the highest accuracy of LLMs\\ncan only reach 60% and 67% for English and Chinese,\\nrespectively. After adding noise, the highest accuracy de-\\ncreases to 43% and 55%. These results suggest that LLMs\\nstruggle with integrating information effectively and are not\\nwell-suited for directly answering complex questions.\\n(2) Complex questions are more challenging for RAG\\nwith noisy documents. Performance decline becomes sig-\\nnificant when the noise ratio is 0.4, but for simple problems,\\na significant decline occurs only at a noise ratio of 0.8 at a\\nsignificance level of 0.05. This indicates that complex prob-\\nlems are more vulnerable to interference from noise. We\\nspeculate that this is because solving complex problems re-\\nquires integrating information from multiple documents, and\\nthis information can be considered as noise to each other,\\nmaking it harder for the model to extract relevant informa-\\ntion from the documents.\\nError Analysis We conducted an error analysis on\\nChatGLM2-6B (noise ratio is 0). Apart from the similar er-\\nrors founded in the noise robustness experiment (38% of the\\ntotal), there are also three types of unique errors. We have\\npresented these cases in Table 6.\\n(1) Merging Error (28% of the total). The model some-\\ntimes merges the answers of the two sub-questions, resulting\\nin an error. It mistakenly uses the answer from one question\\nto address both two questions. At this point, the model will\\ndisregard any documents related to one sub-question. For\\nexample, in Table 6, it incorrectly states that Group D is the\\nWorld Cup group for both France and Germany, while in fact\\nGermany is actually assigned to Group E.\\n(2) Ignoring Error (28% of the total). Sometimes, the\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17759', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='854f3e03-8eaf-4a7f-a8bc-6c7221a674e9', embedding=None, metadata={'page_label': '7', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='English Chinese\\nNoise Ratio 0 0.2\\n0.4 0 0.2\\n0.4\\nChatGPT 55 51\\n34 63 58 47\\nChatGLM-6B 45 36\\n35 60 53\\n52\\nChatGLM2-6B 34 32\\n21 44 43\\n32\\nVicuna-7B-v1.3 60 53\\n43 43 36\\n25\\nQwen-7B-Chat 55 50\\n37 67 56 55\\nBELLE-7B-2M 40 34\\n24 49 41\\n38\\nTable 5: The experimental result of information integration\\nmeasured by accuracy (%) under different noise ratios.\\nQuestion Answer Response Errors\\nWhat groupings\\nare\\nFrance and\\nGermany in Wo-\\nrld Cup 2022?Group\\nD\\nGroup EFrance and\\nGerman\\ny are\\nGroup D ...Merging\\nErr\\nor\\nWho were\\nthe\\nMVP of Super\\nBowl 2022 and\\n2023?Cooper\\nKupp\\nP\\natrick\\nMahomesMVP of\\nSuper\\nBowl LVI was\\nCooper KuppIgnoring\\nError\\nWhat films\\nwon\\nthe 2022 and\\n2023 Academy\\nAwards for Best\\nPicture?COD A\\nEverything\\nEverywher\\ne\\nAll at OnceCOD Aw\\non\\naward for\\nBest Picture\\nat the 95th ...Misali\\ngnment\\nError\\nTable 6: Error cases of information integration, the re-\\nsponses are generated by ChatGLM2-6B. The bold and ital-\\nicized texts represent the answers to two sub-questions.\\nmodel may ignore one of the sub-questions and only answer\\nthe other. This error occurs when the model lacks a complete\\nunderstanding of the problem and fails to recognize that it\\nconsists of multiple sub-problems. As a result, the model\\nonly considers relevant documents for one sub-problem in\\norder to generate an answer, disregarding the question posed\\nby another sub-problem. For example, in Table 6, the model\\nonly provides the answer for the MVP of Super Bowl 2022\\nand does not consider 2023.\\n(3) Misalignment Error (6% of the total). Sometimes,\\nthe model incorrectly identifies the documents for one sub-\\nquestion as the documents for another sub-question, leading\\nto misaligned answers. For example, in Table 6, the third an-\\nswer has two errors: an ignoring error and a misalignment er-\\nror. Firstly, the model only mentioned the Best Picture of the\\n2023 (95th) Academy Awards, completely disregarding the\\n2022 awards. Additionally, it incorrectly stated that “CODA”\\nis the Best Picture of 2023 when it was actually awarded as\\nthe Best Picture in 2022.\\nThe errors mentioned above are primarily caused by the\\nlimited understanding of complex questions, which hinders\\nthe ability to effectively utilize information from different\\nsub-problems. The key lies in improving the model’s rea-\\nsoning capability. One possible solution is to use a chain-of-\\nthought approach to break down complex problems (Zhou\\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-Acc Acc doc ED\\nED∗CR\\nChatGPT -zh 91 17 1 3\\n33.33\\nQwen-7B-Chat-zh 77 12\\n5 4 25.00\\nChatGPT-en 89 9 8\\n7 57.14\\nTable 7: The result of counterfactual robustness. ACC is the\\naccuracy (%) of LLMs without external documents. ACC doc\\nis the accuracy (%) of LLMs with counterfactual documents.\\nED and ED∗are error detection rates evaluated by exact\\nmatching and ChatGPT, respectively. CR is the error cor-\\nrection rate.\\never, these methods slow down the inference speed and can-\\nnot provide timely responses.\\nResults on Counterfactual Robustness Testbed\\nIn order to ensure that LLMs possess relevant knowledge,\\nwe assess their performance by directly asking them ques-\\ntions. However, we found that most LLMs struggle to an-\\nswer them correctly. To ensure a more reasonable evalua-\\ntion, we only consider LLMs that have an accuracy rate of\\nover 70% as this threshold is relatively high and encom-\\npasses more LLMs. The results are shown in Table 7. We\\npresent the following metrics: accuracy without any docu-\\nments, accuracy with counterfactual documents, error de-\\ntection rates, and error correction rates. We can see that It\\nis hard for LLMs to identify and correct factual errors in the\\ndocuments. This suggests that the model can be easily mis-\\nled by documents containing incorrect facts.\\nIt is important to note that retrieval-augmented generation\\nis not designed to automatically address factual errors within\\na given context, as this contradicts the underlying assump-\\ntion that the model lacks knowledge and relies on retrieved\\ndocuments for additional information. However, this issue is\\ncrucial in practical applications due to the abundance of fake\\nnews on the internet. Existing LLMs do not have a safeguard\\nto handle inaccurate responses caused by misinformation. In\\nfact, they heavily depend on the information they retrieve.\\nEven when LLMs contain the internal knowledge about the\\nquestions, they often trust false information that is retrieved.\\nThis presents significant a challenge for the future develop-\\nment of RAG in LLMs.\\nConclusion\\nIn this paper, we evaluated four abilities of retrieval-\\naugmented generation in LLMs: noise robustness, nega-\\ntive rejection, information integration, and counterfactual\\nrobustness. To conduct the evaluation, we built Retrieval-\\nAugmented Generation Benchmark (RGB). The instances of\\nRGB are generated from latest news articles and the external\\ndocuments obtained from search engines. The experimental\\nresults suggest that current LLMs have limitations in the 4\\nabilities. This indicates that there is still a significant amount\\nof work needed to effectively apply RAG to LLMs. To en-\\nsure accurate and reliable responses from LLMs, it is crucial\\nto exercise caution and carefully design for RAG.\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17760', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b5777dc0-e57d-4ee1-85f1-f435730a5035', embedding=None, metadata={'page_label': '8', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Acknowledgements\\nThis research work is supported by the National Natural\\nScience Foundation of China under Grants no. 62122077,\\n62106251, 62306303, the CAS Project for Young Scien-\\ntists in Basic Research under Grant No.YSBR-040 and\\nBeijing Municipal Science & Technology Comission No.\\nZ231100010323002. Xianpei Han is sponsored by CCF-\\nBaiChuan-Ebtech Foundation Model Fund.\\nReferences\\nAdlakha, V .; BehnamGhader, P.; Lu, X. H.; Meade, N.; and\\nReddy, S. 2023. Evaluating Correctness and Faithfulness\\nof Instruction-Following Models for Question Answering.\\narXiv:2307.16877.\\nBai, J.; Bai, S.; Chu, Y .; Cui, Z.; Dang, K.; Deng, X.; Fan,\\nY .; Ge, W.; Han, Y .; Huang, F.; Hui, B.; Ji, L.; Li, M.; Lin,\\nJ.; Lin, R.; Liu, D.; Liu, G.; Lu, C.; Lu, K.; Ma, J.; Men,\\nR.; Ren, X.; Ren, X.; Tan, C.; Tan, S.; Tu, J.; Wang, P.;\\nWang, S.; Wang, W.; Wu, S.; Xu, B.; Xu, J.; Yang, A.; Yang,\\nH.; Yang, J.; Yang, S.; Yao, Y .; Yu, B.; Yuan, H.; Yuan, Z.;\\nZhang, J.; Zhang, X.; Zhang, Y .; Zhang, Z.; Zhou, C.; Zhou,\\nJ.; Zhou, X.; and Zhu, T. 2023. Qwen Technical Report.\\narXiv preprint arXiv:2309.16609.\\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V .; Xu,\\nY .; and Fung, P. 2023. A Multitask, Multilingual, Multi-\\nmodal Evaluation of ChatGPT on Reasoning, Hallucination,\\nand Interactivity. arXiv:2302.04023.\\nBELLEGroup. 2023. BELLE: Be Everyone’s Large\\nLanguage model Engine. https://github.com/LianjiaTech/\\nBELLE. Accessed: 2024-01-10.\\nBian, N.; Liu, P.; Han, X.; Lin, H.; Lu, Y .; He, B.; and\\nSun, L. 2023. A Drop of Ink Makes a Million Think: The\\nSpread of False Information in Large Language Models.\\narXiv:2305.04812.\\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\\nford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;\\nDamoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\\nRing, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\\nCassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\\nO.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\\nSifre, L. 2022. Improving language models by retrieving\\nfrom trillions of tokens. arXiv:2112.04426.\\nCai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and\\nShi, S. 2019a. Skeleton-to-Response: Dialogue Genera-\\ntion Guided by Retrieval Memory. In Proceedings of the\\n2019 Conference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers), 1219–\\n1228. Minneapolis, Minnesota: Association for Computa-\\ntional Linguistics.\\nCai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; and Shi, S.\\n2019b. Retrieval-guided Dialogue Response Generation via\\na Matching-to-Generation Framework. In Proceedings of\\nthe 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-IJCNLP) ,1866–1875. Hong Kong, China: Association for Computa-\\ntional Linguistics.\\nCao, M.; Dong, Y .; Wu, J.; and Cheung, J. C. K. 2020. Fac-\\ntual Error Correction for Abstractive Summarization Mod-\\nels. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP), 6251–\\n6258. Online: Association for Computational Linguistics.\\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Yang, L.; Zhu,\\nK.; Chen, H.; Yi, X.; Wang, C.; Wang, Y .; Ye, W.;\\nZhang, Y .; Chang, Y .; Yu, P. S.; Yang, Q.; and Xie, X.\\n2023. A Survey on Evaluation of Large Language Models.\\narXiv:2307.03109.\\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\\nImpressing GPT-4 with 90%* ChatGPT Quality.\\nCui, J.; Li, Z.; Yan, Y .; Chen, B.; and Yuan, L. 2023. Chat-\\nLaw: Open-Source Legal Large Language Model with Inte-\\ngrated External Knowledge Bases. arXiv:2306.16092.\\nDrozdov, A.; Sch ¨arli, N.; Aky ¨urek, E.; Scales, N.; Song,\\nX.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compo-\\nsitional Semantic Parsing with Large Language Models. In\\nThe Eleventh International Conference on Learning Repre-\\nsentations.\\nEdward Beeching, N. H. S. H. N. L. N. R. O. S. L. T.\\nT. W., Cl ´ementine Fourrier. 2023. Open LLM Leader-\\nboard. https://huggingface.co/spaces/HuggingFaceH4/\\nopen llmleaderboard. Accessed: 2024-01-10.\\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\\nYue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-\\nman Experts? Comparison Corpus, Evaluation, and Detec-\\ntion. arXiv:2301.07597.\\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.\\n2020. REALM: Retrieval-Augmented Language Model Pre-\\nTraining. In Proceedings of the 37th International Confer-\\nence on Machine Learning, ICML’20. JMLR.org.\\nHe, H.; Zhang, H.; and Roth, D. 2022. Rethinking\\nwith Retrieval: Faithful Large Language Model Inference.\\narXiv:2301.00303.\\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\\ntitask Language Understanding. In International Conference\\non Learning Representations.\\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;\\nLiu, J.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He,\\nJ. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese\\nEvaluation Suite for Foundation Models. arXiv preprint\\narXiv:2305.08322.\\nIzacard, G.; and Grave, E. 2021. Leveraging Passage Re-\\ntrieval with Generative Models for Open Domain Ques-\\ntion Answering. In Proceedings of the 16th Conference of\\nthe European Chapter of the Association for Computational\\nLinguistics: Main Volume, 874–880. Online: Association for\\nComputational Linguistics.\\nIzacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni,\\nF.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17761', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7590b2f9-5cd9-4aef-8e80-e40aae4f8510', embedding=None, metadata={'page_label': '9', 'file_name': 'Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'file_type': 'application/pdf', 'file_size': 275198, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Grave, E. 2022. Atlas: Few-shot Learning with Retrieval\\nAugmented Language Models. arXiv:2208.03299.\\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\\nlucination in Natural Language Generation. ACM Comput.\\nSurv., 55(12).\\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;\\nGoyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel,\\nT.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented\\nGeneration for Knowledge-Intensive NLP Tasks. In Pro-\\nceedings of the 34th International Conference on Neural\\nInformation Processing Systems, NIPS’20. Red Hook, NY ,\\nUSA: Curran Associates Inc. ISBN 9781713829546.\\nLi, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\\nVeit, A.; Yu, F.; and Kumar, S. 2023a. Large Language\\nModels with Controllable Working Memory. In Findings of\\nthe Association for Computational Linguistics: ACL 2023 ,\\n1774–1793. Toronto, Canada: Association for Computa-\\ntional Linguistics.\\nLi, X.; Zhang, T.; Dubois, Y .; Taori, R.; Gulrajani, I.;\\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. Al-\\npacaEval: An Automatic Evaluator of Instruction-following\\nModels. https://github.com/tatsu-lab/alpaca eval. Accessed:\\n2024-01-10.\\nLi, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c. Are\\nChatGPT and GPT-4 General-Purpose Solvers for Financial\\nText Analytics? An Examination on Several Typical Tasks.\\narXiv:2305.05862.\\nLiu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifi-\\nability in Generative Search Engines. arXiv:2304.09848.\\nMaynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.\\n2020. On Faithfulness and Factuality in Abstractive Sum-\\nmarization. In Proceedings of the 58th Annual Meeting of\\nthe Association for Computational Linguistics, 1906–1919.\\nOnline: Association for Computational Linguistics.\\nOpenAI. 2022. Chatgpt: Optimizing language models for\\ndialogue. https://openai.com/blog/chatgpt. Accessed: 2024-\\n01-10.\\nPeng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y .; Hu, Y .;\\nHuang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.\\nCheck Your Facts and Try Again: Improving Large Lan-\\nguage Models with External Knowledge and Automated\\nFeedback. arXiv:2302.12813.\\nQin, Y .; Liang, S.; Ye, Y .; Zhu, K.; Yan, L.; Lu, Y .; Lin, Y .;\\nCong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.;\\nZhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023.\\nToolLLM: Facilitating Large Language Models to Master\\n16000+ Real-world APIs. arXiv:2307.16789.\\nRaunak, V .; Menezes, A.; and Junczys-Dowmunt, M. 2021.\\nThe Curious Case of Hallucinations in Neural Machine\\nTranslation. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies, 1172–\\n1183. Online: Association for Computational Linguistics.\\nRen, R.; Wang, Y .; Qu, Y .; Zhao, W. X.; Liu, J.; Tian, H.;\\nWu, H.; Wen, J.-R.; and Wang, H. 2023. Investigating theFactual Knowledge Boundary of Large Language Models\\nwith Retrieval Augmentation. arXiv:2307.11019.\\nShen, X.; Chen, Z.; Backes, M.; and Zhang, Y . 2023. In\\nChatGPT We Trust? Measuring and Characterizing the Re-\\nliability of ChatGPT. arXiv:2304.08979.\\nShi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.;\\nLewis, M.; Zettlemoyer, L.; and tau Yih, W. 2023. RE-\\nPLUG: Retrieval-Augmented Black-Box Language Models.\\narXiv:2301.12652.\\nTHUDM. 2023a. ChatGLM-6B. https://github.com/\\nTHUDM/ChatGLM-6B. Accessed: 2024-01-10.\\nTHUDM. 2023b. ChatGLM2-6B. https://github.com/\\nTHUDM/ChatGLM2-6B. Accessed: 2024-01-10.\\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\\nA. 2023. Interleaving Retrieval with Chain-of-Thought Rea-\\nsoning for Knowledge-Intensive Multi-Step Questions. In\\nProceedings of the 61st Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long Papers) ,\\n10014–10037. Toronto, Canada: Association for Computa-\\ntional Linguistics.\\nWang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.;\\nMichael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. Su-\\nperGLUE: A Stickier Benchmark for General-Purpose Lan-\\nguage Understanding Systems. Red Hook, NY , USA: Curran\\nAssociates Inc.\\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\\nBowman, S. R. 2019b. GLUE: A Multi-Task Benchmark\\nand Analysis Platform for Natural Language Understanding.\\nInInternational Conference on Learning Representations.\\nXu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;\\nGao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;\\nand Zhou, J. 2023a. CValues: Measuring the Values of Chi-\\nnese Large Language Models from Safety to Responsibility.\\narXiv:2307.09705.\\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-\\nS. 2023b. Search-in-the-Chain: Towards Accurate, Credi-\\nble and Traceable Large Language Models for Knowledge-\\nintensive Tasks. arXiv:2304.14732.\\nZhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y . K.; and Bing,\\nL. 2023. M3Exam: A Multilingual, Multimodal, Multilevel\\nBenchmark for Examining Large Language Models.\\nZhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang,\\nY .; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:\\nA Human-Centric Benchmark for Evaluating Foundation\\nModels. arXiv:2304.06364.\\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and\\nChi, E. H. 2023a. Least-to-Most Prompting Enables Com-\\nplex Reasoning in Large Language Models. In The Eleventh\\nInternational Conference on Learning Representations.\\nZhou, S.; Alon, U.; Xu, F. F.; Jiang, Z.; and Neubig, G.\\n2023b. DocPrompting: Generating Code by Retrieving the\\nDocs. In The Eleventh International Conference on Learn-\\ning Representations.\\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\\n17762', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea1b0d76-6926-48db-b8ef-7d994c6eb469', embedding=None, metadata={'page_label': '1', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n∗Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b5559ce-4498-49c4-96fe-2c3d7c9be003', embedding=None, metadata={'page_label': '2', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ered relevant contexts, which are fetched from\\nPLMs and provide richer background information\\n(Table 2). For example, by prompting a PLM\\nto generate the title of a relevant passage given\\na query and appending the generated title to the\\nquery, it becomes easier to retrieve that relevant\\npassage. Intuitively, the generated contexts\\nexplicitly express the search intent not presented\\nin the original query. As a result, GARwith\\nsparse representations achieves comparable or\\neven better performance than state-of-the-art\\napproaches (Karpukhin et al., 2020; Guu et al.,\\n2020) with dense representations of the original\\nqueries, while being more lightweight and efﬁcient\\nin terms of both training and inference (including\\nthe cost of the generation model) (Sec. 6.4).\\nSpeciﬁcally, we expand the query (question) by\\nadding relevant contexts as follows. We conduct\\nseq2seq learning with the question as the input\\nand various freely accessible in-domain contexts as\\nthe output such as the answer, the sentence where\\nthe answer belongs to , and the title of a passage\\nthat contains the answer . We then append the gen-\\nerated contexts to the question as the generation-\\naugmented query for retrieval. We demonstrate\\nthat using multiple contexts from diverse gener-\\nation targets is beneﬁcial as fusing the retrieval\\nresults of different generation-augmented queries\\nconsistently yields better retrieval accuracy.\\nWe conduct extensive experiments on the Nat-\\nural Questions (NQ) (Kwiatkowski et al., 2019)\\nand TriviaQA (Trivia) (Joshi et al., 2017) datasets.\\nThe results reveal four major advantages of GAR:\\n(1)GAR, combined with BM25, achieves signif-\\nicant gains over the same BM25 model that uses\\nthe original queries or existing unsupervised query\\nexpansion (QE) methods. (2) GARwith sparse rep-\\nresentations (BM25) achieves comparable or even\\nbetter performance than the current state-of-the-art\\nretrieval methods, such as DPR (Karpukhin et al.,\\n2020), that use dense representations. (3) Since\\nGARuses sparse representations to measure lexical\\noverlap2, it is complementary to dense representa-\\ntions: by fusing the retrieval results of GARand\\nDPR (denoted as GAR+), we obtain consistently\\nbetter performance than either method used individ-\\nually. (4) GARoutperforms DPR in the end-to-end\\nQA performance (EM) when the same extractive\\nreader is used: EM=41.8 (43.8 for GAR+) on NQ\\n2Strictly speaking, GARwith sparse representations han-\\ndles semantics before retrieval by enriching the queries, while\\nmaintaining the advantage of exact matching.and 62.7 on Trivia, creating new state-of-the-art re-\\nsults for extractive OpenQA. GARalso outperforms\\nother retrieval methods under the generative setup\\nwhen the same generative reader is used: EM=38.1\\n(45.3 for G AR+) on NQ and 62.2 on Trivia.\\nContributions . (1) We propose Generation-\\nAugmented Retrieval ( GAR), which augments\\nqueries with heuristically discovered relevant con-\\ntexts through text generation without external su-\\npervision or time-consuming downstream feedback.\\n(2) We show that using generation-augmented\\nqueries achieves signiﬁcantly better retrieval and\\nQA results than using the original queries or ex-\\nisting unsupervised QE methods. (3) We show\\nthatGAR, combined with a simple BM25 model,\\nachieves new state-of-the-art performance on two\\nbenchmark datasets in extractive OpenQA and com-\\npetitive results in the generative setting.\\n2 Related Work\\nConventional Query Expansion .GARshares\\nsome merits with query expansion (QE) meth-\\nods based on pseudo relevance feedback (Rocchio,\\n1971; Abdul-Jaleel et al., 2004; Lv and Zhai, 2010)\\nin that they both expand the queries with relevant\\ncontexts (terms) without the use of external super-\\nvision. GARis superior as it expands the queries\\nwith knowledge stored in the PLMs rather than\\nthe retrieved passages and its expanded terms are\\nlearned through text generation.\\nRecent Query Reformulation . There are recent\\nor concurrent studies (Nogueira and Cho, 2017;\\nZaiem and Sadat, 2019; Yu et al., 2020; Vaku-\\nlenko et al., 2020; Lin et al., 2020) that reformu-\\nlate queries with generation models for other re-\\ntrieval tasks. However, these studies are not eas-\\nily applicable or efﬁcient enough for OpenQA be-\\ncause: (1) They require external resources such as\\nparaphrase data (Zaiem and Sadat, 2019), search\\nsessions (Yu et al., 2020), or conversational con-\\ntexts (Lin et al., 2020; Vakulenko et al., 2020)\\nto form the reformulated queries, which are not\\navailable or showed inferior domain-transfer per-\\nformance in OpenQA (Zaiem and Sadat, 2019);\\n(2) They involve time-consuming training process\\nsuch as RL. For example, Nogueira and Cho (2017)\\nreported a training time of 8 to 10 days as it uses\\nretrieval performance in the reward function and\\nconducts retrieval at each iteration. In contrast,\\nGARuses freely accessible in-domain contexts like\\npassage titles as the generation targets and standard', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b27827d-09fc-445d-9d43-de5b997ae239', embedding=None, metadata={'page_label': '3', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='seq2seq learning, which, despite its simplicity, is\\nnot only more efﬁcient but effective for OpenQA.\\nRetrieval for OpenQA . Existing sparse retrieval\\nmethods for OpenQA (Chen et al., 2017) solely rely\\non the information of the questions. GARextends\\nto contexts relevant to the questions by extracting\\ninformation inside PLMs and helps sparse meth-\\nods achieve comparable or better performance than\\ndense methods (Guu et al., 2020; Karpukhin et al.,\\n2020), while enjoying the simplicity and efﬁciency\\nof sparse representations. GARcan also be used\\nwith dense representations to seek for even better\\nperformance, which we leave as future work.\\nGenerative QA . Generative QA generates answers\\nthrough seq2seq learning instead of extracting an-\\nswer spans. Recent studies on generative OpenQA\\n(Lewis et al., 2020a; Min et al., 2020; Izacard and\\nGrave, 2020) are orthogonal to GARin that they\\nfocus on improving the reading stage and directly\\nreuse DPR (Karpukhin et al., 2020) as the retriever.\\nUnlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conﬁrms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciﬁed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D⊂Cand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efﬁ-\\nciency of the retriever and consequently improve\\nthe performance of the reader.\\n3.2 Generation of Query Contexts\\nInGAR, queries are augmented with various heuris-\\ntically discovered relevant contexts in order to re-\\ntrieve more relevant passages in terms of both quan-\\ntity and quality. For the task of OpenQA where the\\nquery is a question, we take the following threefreely accessible contexts as the generation targets.\\nWe show in Sec. 6.2 that having multiple gener-\\nation targets is helpful in that fusing their results\\nconsistently brings better retrieval accuracy.\\nContext 1: The default target (answer) . The de-\\nfault target is the label in the task of interest, which\\nis the answer in OpenQA. The answer to the ques-\\ntion is apparently useful for the retrieval of relevant\\npassages that contain the answer itself. As shown\\nin previous work (Roberts et al., 2020; Brown et al.,\\n2020), PLMs are able to answer certain questions\\nsolely by taking the questions as input ( i.e., closed-\\nbook QA). Instead of using the generated answers\\ndirectly as in closed-book QA, GARtreats them\\nas contexts of the question for retrieval. The ad-\\nvantage is that even if the generated answers are\\npartially correct (or even incorrect), they may still\\nbeneﬁt retrieval as long as they are relevant to the\\npassages that contain the correct answers ( e.g., co-\\noccur with the correct answers).\\nContext 2: Sentence containing the default tar-\\nget. The sentence in a passage that contains the\\nanswer is used as another generation target. Sim-\\nilar to using answers as the generation target, the\\ngenerated sentences are still beneﬁcial for retriev-\\ning relevant passages even if they do not contain\\nthe answers, as their semantics is highly related to\\nthe questions/answers (examples in Sec. 6.1). One\\ncan take the relevant sentences in the ground-truth\\npassages (if any) or those in the positive passages\\nof a retriever as the reference, depending on the\\ntrade-off between reference quality and diversity.\\nContext 3: Title of passage containing the de-\\nfault target . One can also use the titles of rele-\\nvant passages as the generation target if available.\\nSpeciﬁcally, we retrieve Wikipedia passages using\\nBM25 with the question as the query, and take the\\npage titles of positive passages that contain the an-\\nswers as the generation target. We observe that\\nthe page titles of positive passages are often entity\\nnames of interest, and sometimes (but not always)\\nthe answers to the questions. Intuitively, if GAR\\nlearns which Wikipedia pages the question is re-\\nlated to, the queries augmented by the generated\\ntitles would naturally have a better chance of re-\\ntrieving those relevant passages.\\nWhile it is likely that some of the generated\\nquery contexts involve unfaithful or nonfactual in-\\nformation due to hallucination in text generation\\n(Mao et al., 2020) and introduce noise during re-\\ntrieval, they are beneﬁcial rather than harmful over-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b8b14968-08d0-4089-8300-8cbd95b83cc7', embedding=None, metadata={'page_label': '4', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='all, as our experiments show that GARimprove\\nboth retrieval and QA performance over BM25 sig-\\nniﬁcantly. Also, since we generate 3 different (com-\\nplementary) query contexts and fuse their retrieval\\nresults, the distraction of hallucinated content is\\nfurther alleviated.\\n3.3 Retrieval with Generation-Augmented\\nQueries\\nAfter generating the contexts of a query, we append\\nthem to the query to form a generation-augmented\\nquery .3We observe that conducting retrieval with\\nthe generated contexts ( e.g., answers) alone as\\nqueries instead of concatenation is ineffective be-\\ncause (1) some of the generated answers are rather\\nirrelevant, and (2) a query consisting of the correct\\nanswer alone (without the question) may retrieve\\nfalse positive passages with unrelated contexts that\\nhappen to contain the answer. Such low-quality\\npassages may lead to potential issues in the follow-\\ning passage reading stage.\\nIf there are multiple query contexts, we conduct\\nretrieval using queries with different generated con-\\ntexts separately and then fuse their results. The per-\\nformance of one-time retrieval with all the contexts\\nappended is slightly but not signiﬁcantly worse.\\nFor simplicity, we fuse the retrieval results in a\\nstraightforward way: an equal number of passages\\nare taken from the top-retrieved passages of each\\nsource. One may also use weighted or more so-\\nphisticated fusion strategies such as reciprocal rank\\nfusion (Cormack et al., 2009), the results of which\\nare slightly better according to our experiments.4\\nNext, one can use any off-the-shelf retriever for\\npassage retrieval. Here, we use a simple BM25\\nmodel to demonstrate that GARwith sparse repre-\\nsentations can already achieve comparable or better\\nperformance than state-of-the-art dense methods\\nwhile being more lightweight and efﬁcient (includ-\\ning the cost of the generation model), closing the\\ngap between sparse and dense retrieval methods.\\n4 OpenQA with G AR\\nTo further verify the effectiveness of GAR, we\\nequip it with both extractive and generative read-\\ners for end-to-end QA evaluation. We follow the\\n3One may create a title ﬁeld during document indexing\\nand conduct multi-ﬁeld retrieval but here we append the titles\\nto the questions as other query contexts for generalizability.\\n4We use the fusion tools at https://github.com/\\njoaopalotti/trectools .reader design of the major baselines for a fair com-\\nparison, while virtually any existing QA reader can\\nbe used with G AR.\\n4.1 Extractive Reader\\nFor the extractive setup, we largely follow the de-\\nsign of the extractive reader in DPR (Karpukhin\\net al., 2020). Let D= [d1, d2, ..., d k]denote the list\\nof retrieved passages with passage relevance scores\\nD. LetSi= [s1, s2, ..., s N]denote the top Ntext\\nspans in passage diranked by span relevance scores\\nSi. Brieﬂy, the DPR reader uses BERT-base (De-\\nvlin et al., 2019) for representation learning, where\\nit estimates the passage relevance score Dkfor\\neach retrieved passage dkbased on the [CLS] to-\\nkens of all retrieved passages D, and assigns span\\nrelevance scores Sifor each candidate span based\\non the representations of its start and end tokens.\\nFinally, the span with the highest span relevance\\nscore from the passage with the highest passage rel-\\nevance score is chosen as the answer. We refer the\\nreaders to Karpukhin et al. (2020) for more details.\\nPassage-level Span Voting . Many extractive QA\\nmethods (Chen et al., 2017; Min et al., 2019b; Guu\\net al., 2020; Karpukhin et al., 2020) measure the\\nprobability of span extraction in different retrieved\\npassages independently, despite that their collec-\\ntive signals may provide more evidence in deter-\\nmining the correct answer. We propose a simple\\nyet effective passage-level span voting mechanism,\\nwhich aggregates the predictions of the spans in\\nthe same surface form from different retrieved pas-\\nsages. Intuitively, if a text span is considered as the\\nanswer multiple times in different passages, it is\\nmore likely to be the correct answer. Speciﬁcally,\\nGARcalculates a normalized score p(Si[j])for the\\nj-th span in passage diduring inference as follows:\\np(Si[j]) = softmax (D)[i]×softmax (Si)[j].GAR\\nthen aggregates the scores of the spans with the\\nsame surface string among all the retrieved pas-\\nsages as the collective passage-level score.5\\n4.2 Generative Reader\\nFor the generative setup, we use a seq2seq frame-\\nwork where the input is the concatenation of the\\nquestion and top-retrieved passages and the target\\noutput is the desired answer. Such generative read-\\ners are adopted in recent methods such as SpanSe-\\n5We ﬁnd that the number of spans used for normalization\\nin each passage does not have signiﬁcant impact on the ﬁnal\\nperformance (we take N= 5) and using the raw or normalized\\nstrings for aggregation also perform similarly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ccce9615-1f41-41c5-ad57-01ae13983cd6', embedding=None, metadata={'page_label': '5', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='qGen (Min et al., 2020) and Longformer (Belt-\\nagy et al., 2020). Speciﬁcally, we use BART-large\\n(Lewis et al., 2019) as the generative reader, which\\nconcatenates the question and top-retrieved pas-\\nsages up to its length limit (1,024 tokens, 7.8 pas-\\nsages on average). Generative GARis directly com-\\nparable with SpanSeqGen (Min et al., 2020) that\\nuses the retrieval results of DPR but not comparable\\nwith Fusion-in-Decoder (FID) (Izacard and Grave,\\n2020) since it encodes 100 passages rather than\\n1,024 tokens and involves more model parameters.\\n5 Experiment Setup\\n5.1 Datasets\\nWe conduct experiments on the open-domain ver-\\nsion of two popular QA benchmarks: Natural Ques-\\ntions (NQ) (Kwiatkowski et al., 2019) and Trivi-\\naQA (Trivia) (Joshi et al., 2017). The statistics of\\nthe datasets are listed in Table 1.\\nDataset Train / Val / Test Q-len A-len #-A\\nNQ 79,168 / 8,757 / 3,610 12.5 5.2 1.2\\nTrivia 78,785 / 8,837 / 11,313 20.2 5.5 13.7\\nTable 1: Dataset statistics that show the number of sam-\\nples per data split, the average question (answer) length,\\nand the number of answers for each question.\\n5.2 Evaluation Metrics\\nFollowing prior studies (Karpukhin et al., 2020),\\nwe use top-k retrieval accuracy to evaluate the per-\\nformance of the retriever and the Exact Match (EM)\\nscore to measure the performance of the reader.\\nTop-k retrieval accuracy is deﬁned as the pro-\\nportion of questions for which the top-k retrieved\\npassages contain at least one answer span, which\\nis an upper bound of how many questions are “an-\\nswerable” by an extractive reader.\\nExact Match (EM) is the proportion of the pre-\\ndicted answer spans being exactly the same as (one\\nof) the ground-truth answer(s), after string normal-\\nization such as article and punctuation removal.\\n5.3 Compared Methods\\nFor passage retrieval, we mainly compare with\\nBM25 and DPR, which represent the most used\\nstate-of-the-art methods of sparse and dense re-\\ntrieval for OpenQA, respectively. For query ex-\\npansion, we re-emphasize that GARis the ﬁrst QE\\napproach designed for OpenQA and most of the\\nrecent approaches are not applicable or efﬁcientenough for OpenQA since they have task-speciﬁc\\nobjectives, require external supervision that was\\nshown to transfer poorly to OpenQA, or take many\\ndays to train (Sec. 2). We thus compare with a clas-\\nsic unsupervised QE method RM3 (Abdul-Jaleel\\net al., 2004) that does not need external resources\\nfor a fair comparison. For passage reading, we\\ncompare with both extractive (Min et al., 2019a;\\nAsai et al., 2019; Lee et al., 2019; Min et al., 2019b;\\nGuu et al., 2020; Karpukhin et al., 2020) and gen-\\nerative (Brown et al., 2020; Roberts et al., 2020;\\nMin et al., 2020; Lewis et al., 2020a; Izacard and\\nGrave, 2020) methods when equipping GARwith\\nthe corresponding reader.\\n5.4 Implementation Details\\nRetriever . We use Anserini (Yang et al., 2017) for\\ntext retrieval of BM25 and GARwith its default\\nparameters. We conduct grid search for the QE\\nbaseline RM3 (Abdul-Jaleel et al., 2004).\\nGenerator . We use BART-large (Lewis et al.,\\n2019) to generate query contexts in GAR. When\\nthere are multiple desired targets (such as multi-\\nple answers or titles), we concatenate them with\\n[SEP] tokens as the reference and remove the [SEP]\\ntokens in the generation-augmented queries. For\\nTrivia, in particular, we use the value ﬁeld as the\\ngeneration target of answer and observe better per-\\nformance. We take the checkpoint with the best\\nROUGE-1 F1 score on the validation set, while\\nobserving that the retrieval accuracy of GARis rel-\\natively stable to the checkpoint selection since we\\ndo not directly use the generated contexts but treat\\nthem as augmentation of queries for retrieval.\\nReader . Extractive GARuses the reader of DPR\\nwith largely the same hyperparameters, which is\\ninitialized with BERT-base (Devlin et al., 2019)\\nand takes 100 (500) retrieved passages during train-\\ning (inference). Generative GARconcatenates the\\nquestion and top-10 retrieved passages, and takes\\nat most 1,024 tokens as input. Greedy decoding is\\nadopted for all generation models, which appears to\\nperform similarly to (more expensive) beam search.\\n6 Experiment Results\\nWe evaluate the effectiveness of GARin three\\nstages: generation of query contexts (Sec. 6.1),\\nretrieval of relevant passages (Sec. 6.2), and pas-\\nsage reading for OpenQA (Sec. 6.3). Ablation\\nstudies are mostly shown on the NQ dataset to un-\\nderstand the drawbacks of GARsince it achieves', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed1313ee-4b02-49f5-b36b-0f316fd98416', embedding=None, metadata={'page_label': '6', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Question : when did bat out of hell get released?\\nAnswer :September 1977 {September 1977}\\nSentence : Bat Out of Hell is the second studio album and the major - label debut by American rock singer Meat\\nLoaf ... released in September 1977 on Cleveland International / Epic Records.\\n{The album was released in September 1977 on Cleveland International / Epic Records. }\\nTitle :Bat Out of Hell {Bat Out of Hell}\\nQuestion : who sings does he love me with reba?\\nAnswer :Brooks & Dunn {Linda Davis}\\nSentence :Linda Kaye Davis ( born November 26, 1962 ) is an American country music singer.\\n{“ Does He Love You ” is a song written by Sandy Knox and Billy Stritch, and recorded as a duet by American\\ncountry music artists Reba McEntire and Linda Davis. }\\nTitle :Does He Love Me [SEP] Does He Love Me (Reba McEntire song) [SEP] I Do (Reba McEntire album)\\n{Linda Davis [SEP] Greatest Hits V olume Two (Reba McEntire album) [SEP] Does He Love You }\\nQuestion : what is the name of wonder womans mother?\\nAnswer :Mother Magda {Queen Hippolyta}\\nSentence : In the Amazonian myths, she is the daughter of the Amazon queen Sifrat and the male dwarf Shuri,\\nand is the mother of Wonder Woman. {Wonder Woman’s origin story relates that she was sculpted from clay\\nby her mother Queen Hippolyta and given life by Aphrodite. }\\nTitle :Wonder Woman [SEP] Diana Prince [SEP] Wonder Woman (2011 TV pilot)\\n{Wonder Woman [SEP] Orana (comics) [SEP] Wonder Woman (TV series) }\\nTable 2: Examples of generated query contexts .Relevant andirrelevant contexts are shown in green and\\nred. Ground-truth references are shown in the {braces}. The issue of generating wrong answers is alleviated by\\ngenerating other contexts highly related to the question/answer.\\nbetter performance on Trivia.\\n6.1 Query Context Generation\\nAutomatic Evaluation . To evaluate the quality\\nof the generated query contexts, we ﬁrst measure\\ntheir lexical overlap with the ground-truth query\\ncontexts. As suggested by the nontrivial ROUGE\\nscores in Table 3, GARdoes learn to generate\\nmeaningful query contexts that could help the re-\\ntrieval stage. We next measure the lexical overlap\\nbetween the query and the ground-truth passage.\\nThe ROUGE-1/2/L F1 scores between the original\\nquery and ground-truth passage are 6.00/2.36/5.01,\\nand those for the generation-augmented query are\\n7.05/2.84/5.62 (answer), 13.21/6.99/10.27 (sen-\\ntence), 7.13/2.85/5.76 (title) on NQ, respectively.\\nSuch results further demonstrate that the generated\\nquery contexts signiﬁcantly increase the word over-\\nlap between the queries and the positive passages,\\nand thus are likely to improve retrieval results.6\\nCase Studies . In Table 2, we show several ex-\\namples of the generated query contexts and their\\nground-truth references. In the ﬁrst example, the\\ncorrect album release date appears in both the gen-\\nerated answer and the generated sentence, and the\\ngenerated title is the same as the Wikipedia page\\n6We use F1 instead of recall to avoid the unfair favor of\\n(longer) generation-augmented query.Context ROUGE-1 ROUGE-2 ROUGE-L\\nAnswer 33.51 20.54 33.30\\nSentence 37.14 24.71 33.91\\nTitle 43.20 32.11 39.67\\nTable 3: ROUGE F1 scores of the generated query\\ncontexts on the validation set of the NQ dataset.\\ntitle of the album. In the last two examples, the\\ngenerated answers are wrong but fortunately, the\\ngenerated sentences contain the correct answer and\\n(or) other relevant information and the generated\\ntitles are highly related to the question as well,\\nwhich shows that different query contexts are com-\\nplementary to each other and the noise during query\\ncontext generation is thus reduced.\\n6.2 Generation-Augmented Retrieval\\nComparison w. the state-of-the-art . We next\\nevaluate the effectiveness of GARfor retrieval.\\nIn Table 4, we show the top-k retrieval accuracy\\nof BM25, BM25 with query expansion (+RM3)\\n(Abdul-Jaleel et al., 2004), DPR (Karpukhin et al.,\\n2020), G AR, and G AR+(GAR+DPR).\\nOn the NQ dataset, while BM25 clearly under-\\nperforms DPR regardless of the number of retrieved\\npassages, the gap between GARand DPR is signiﬁ-\\ncantly smaller and negligible when k≥100. When\\nk≥500,GARis slightly better than DPR despite', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc4dc1a1-e5c8-47f9-9bed-049c2621a2c2', embedding=None, metadata={'page_label': '7', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='MethodNQ Trivia\\nTop-5 Top-20 Top-100 Top-500 Top-1000 Top-5 Top-20 Top-100 Top-500 Top-1000\\nBM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9\\nBM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9\\nDPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 - -\\nGAR 60.9 74.4 85.3 90.3 91.7 73.1 80.4 85.7 88.9 89.7\\nGAR+70.7 81.6 88.9 92.0 93.2 76.0 82.1 86.6 - -\\nTable 4: Top-k retrieval accuracy on the test sets . The baselines are evaluated by ourselves and better than\\nreported in Karpukhin et al. (2020). G ARhelps BM25 to achieve comparable or better performance than DPR.\\nBest and second best methods are bold and underlined , respectively.\\nthat it simply uses BM25 for retrieval. In con-\\ntrast, the classic QE method RM3, while showing\\nmarginal improvement over the vanilla BM25, does\\nnot achieve comparable performance with GARor\\nDPR. By fusing the results of GARand DPR in\\nthe same way as described in Sec. 3.3, we further\\nobtain consistently higher performance than both\\nmethods, with top-100 accuracy 88.9% and top-\\n1000 accuracy 93.2%.\\nOn the Trivia dataset, the results are even more\\nencouraging – GARachieves consistently better\\nretrieval accuracy than DPR when k≥5. On\\nthe other hand, the difference between BM25 and\\nBM25 +RM3 is negligible, which suggests that\\nnaively considering top-ranked passages as relevant\\n(i.e., pseudo relevance feedback) for QE does not\\nalways work for OpenQA. Results on more cutoffs\\nofkcan be found in App. A.\\nEffectiveness of diverse query contexts . In\\nFig. 1, we show the performance of GARwhen\\ndifferent query contexts are used to augment the\\nqueries. Although the individual performance\\nwhen using each query context is somewhat similar,\\nfusing their retrieved passages consistently leads\\nto better performance, conﬁrming that different\\ngeneration-augmented queries are complementary\\nto each other (recall examples in Table 2).\\nPerformance breakdown by question type . In\\nTable 5, we show the top-100 accuracy of the com-\\npared retrieval methods per question type on the\\nNQ test set. Again, GARoutperforms BM25 on all\\ntypes of questions signiﬁcantly and GAR+achieves\\nthe best performance across the board, which fur-\\nther veriﬁes the effectiveness of G AR.\\n6.3 Passage Reading with G AR\\nComparison w. the state-of-the-art . We show\\nthe comparison of end-to-end QA performance of\\nextractive and generative methods in Table 6. Ex-\\ntractive GARachieves state-of-the-art performance\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages30405060708090Top-k Accuracy (%)\\nAnswer+Sentence+Title\\nAnswer+Sentence\\nAnswer+Title\\nAnswer\\nTitle\\nSentenceFigure 1: Top-k retrieval accuracy on the test\\nset of NQ when fusing retrieval results of different\\ngeneration-augmented queries.\\nType Percentage BM25 DPR G AR GAR+\\nWho 37.5% 82.1 88.0 87.5 90.8\\nWhen 19.0% 73.1 86.9 83.8 88.6\\nWhat 15.0% 76.5 82.6 81.5 86.0\\nWhere 10.9% 77.4 89.1 87.0 90.8\\nOther 9.1% 79.3 78.1 81.8 84.2\\nHow 5.0% 78.2 83.8 83.2 85.5\\nWhich 3.3% 89.0 90.7 94.1 94.9\\nWhy 0.3% 90.0 90.0 90.0 90.0\\nTable 5: Top-100 retrieval accuracy breakdown of\\nquestion type on NQ . Best and second best methods\\nin each category are bold and underlined , respectively.\\namong extractive methods on both NQ and Trivia\\ndatasets, despite that it is more lightweight and\\ncomputationally efﬁcient. Generative GARoutper-\\nforms most of the generative methods on Trivia but\\ndoes not perform as well on NQ, which is some-\\nwhat expected and consistent with the performance\\nat the retrieval stage, as the generative reader only\\ntakes a few passages as input and GARdoes not\\noutperform dense retrieval methods on NQ when k\\nis very small. However, combining GARwith DPR\\nachieves signiﬁcantly better performance than both', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8aa1730d-e7e5-4373-a955-6dc19389cf6d', embedding=None, metadata={'page_label': '8', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Method NQ TriviaExtractiveHard EM (Min et al., 2019a) 28.1 50.9 -\\nPath Retriever (Asai et al., 2019) 32.6 - -\\nORQA (Lee et al., 2019) 33.3 45.0 -\\nGraph Retriever (Min et al., 2019b) 34.5 56.0 -\\nREALM (Guu et al., 2020) 40.4 - -\\nDPR (Karpukhin et al., 2020) 41.5 57.9 -\\nBM25 (ours) 37.7 60.1 -\\nGAR 41.8 62.7 74.8\\nGAR+43.8 - -GenerativeGPT-3 (Brown et al., 2020) 29.9 - 71.2\\nT5 (Roberts et al., 2020) 36.6 60.5 -\\nSpanSeqGen (Min et al., 2020) 42.2 - -\\nRAG (Lewis et al., 2020a) 44.5 56.1 68.0\\nFID (Izacard and Grave, 2020) 51.4 67.6 80.1\\nBM25 (ours) 35.3 58.6 -\\nGAR 38.1 62.2 -\\nGAR+45.3 - -\\nTable 6: End-to-end comparison with the state-of-\\nthe-art methods in EM . For Trivia, the left column\\ndenotes the open-domain test set and the right is the\\nhidden Wikipedia test set on the public leaderboard.\\nmethods or baselines that use DPR as input such as\\nSpanSeqGen (Min et al., 2020) and RAG (Lewis\\net al., 2020a). Also, GARoutperforms BM25 sig-\\nniﬁcantly under both extractive and generative se-\\ntups, which again shows the effectiveness of the\\ngenerated query contexts, even if they are heuristi-\\ncally discovered without any external supervision.\\nThe best performing generative method FID\\n(Izacard and Grave, 2020) is not directly compara-\\nble as it takes more (100) passages as input. As an\\nindirect comparison, GARperforms better than FID\\nwhen FID encodes 10 passages (cf. Fig. 2 in Izac-\\nard and Grave (2020)). Moreover, since FID relies\\non the retrieval results of DPR as well, we believe\\nthat it is a low-hanging fruit to replace its input\\nwith GARorGAR+and further boost the perfor-\\nmance.7We also observe that, perhaps surprisingly,\\nextractive BM25 performs reasonably well, espe-\\ncially on the Trivia dataset, outperforming many\\nrecent state-of-the-art methods.8Generative BM25\\nalso performs competitively in our experiments.\\nModel Generalizability . Recent studies (Lewis\\net al., 2020b) show that there are signiﬁcant ques-\\ntion and answer overlaps between the training and\\ntest sets of popular OpenQA datasets. Speciﬁcally,\\n60% to 70% test-time answers also appear in the\\n7This claim is later veriﬁed by the best systems in the\\nNeurIPS 2020 EfﬁcientQA competition (Min et al., 2021).\\n8We ﬁnd that taking 500 passages during reader inference\\ninstead of 100 as in Karpukhin et al. (2020) improves the\\nperformance of BM25 but not DPR.training set and roughly 30% test-set questions\\nhave a near-duplicate paraphrase in the training\\nset. Such observations suggest that many questions\\nmight have been answered by simple question or\\nanswer memorization. To further examine model\\ngeneralizability, we study the per-category perfor-\\nmance of different methods using the annotations\\nin Lewis et al. (2020b).\\nMethod TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nDPR 41.3 69.4 34.6 19.3\\nGAR+(E) 43.8 66.7 38.1 23.9\\nBART 26.5 67.6 10.2 0.8\\nRAG 44.5 70.7 34.9 24.8\\nGAR+(G) 45.3 67.9 38.1 27.0\\nTable 7: EM scores with question-answer overlap\\ncategory breakdown on NQ. (E) and (G) denote ex-\\ntractive and generative readers, respectively. Results of\\nbaseline methods are taken from Lewis et al. (2020b).\\nThe observations on Trivia are similar and omitted.\\nAs listed in Table 7, for the No Overlap category,\\nGAR+(E) outperforms DPR on the extractive setup\\nandGAR+(G) outperforms RAG on the generative\\nsetup, which indicates that better end-to-end model\\ngeneralizability can be achieved by adding GAR\\nfor retrieval. GAR+also achieves the best EM un-\\nder the Answer Overlap Only category. In addition,\\nwe observe that a closed-book BART model that\\nonly takes the question as input performs much\\nworse than additionally taking top-retrieved pas-\\nsages, i.e.,GAR+(G), especially on the questions\\nthat require generalizability. Notably, all methods\\nperform signiﬁcantly better on the Question Over-\\nlapcategory, which suggests that the high Total\\nEM is mostly contributed by question memoriza-\\ntion. That said, GAR+appears to be less dependent\\non question memorization given its lower EM for\\nthis category.9\\n6.4 Efﬁciency of G AR\\nGARis efﬁcient and scalable since it uses sparse\\nrepresentations for retrieval and does not in-\\nvolve time-consuming training process such as\\nRL (Nogueira and Cho, 2017; Liu et al., 2019).\\nThe only overhead of GARis on the generation of\\nquery contexts and the retrieval with generation-\\n9The same ablation study is also conducted on the retrieval\\nstage and similar results are observed. More detailed discus-\\nsions can be found in App. A.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01693960-7755-4556-9672-ce66a185582b', embedding=None, metadata={'page_label': '9', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Training Indexing Retrieval\\nDPR 24h w. 8 GPUs 17.3h w. 8 GPUs 30 min w. 1 GPU\\nGAR 3∼6h w. 1 GPU 0.5h w. 35 CPUs 5 min w. 35 CPUs\\nTable 8: Comparison of computational cost between\\nDPR and G ARat different stages. The training time\\nof G ARis for one generation target but different gener-\\nators can be trained in parallel.\\naugmented (thus longer) queries, whose computa-\\ntional complexity is signiﬁcantly lower than other\\nmethods with comparable retrieval accuracy.\\nWe use Nvidia V100 GPUs and Intel Xeon Plat-\\ninum 8168 CPUs in our experiments. As listed in\\nTable 8, the training time of GARis 3 to 6 hours\\non 1 GPU depending on the generation target. As\\na comparison, REALM (Guu et al., 2020) uses\\n64 TPUs to train for 200k steps during pre-training\\nalone and DPR (Karpukhin et al., 2020) takes about\\n24 hours to train with 8 GPUs. To build the indices\\nof Wikipedia passages, GARonly takes around 30\\nmin with 35 CPUs, while DPR takes 8.8 hours\\non 8 GPUs to generate dense representations and\\nanother 8.5 hours to build the FAISS index (John-\\nson et al., 2017). For retrieval, GARtakes about\\n1 min to generate one query context with 1 GPU,\\n1 min to retrieve 1,000 passages for the NQ test\\nset with answer/title-augmented queries and 2 min\\nwith sentence-augmented queries using 35 CPUs.\\nIn contrast, DPR takes about 30 min on 1 GPU.\\n7 Conclusion\\nIn this work, we propose Generation-Augmented\\nRetrieval and demonstrate that the relevant contexts\\ngenerated by PLMs without external supervision\\ncan signiﬁcantly enrich query semantics and im-\\nprove retrieval accuracy. Remarkably, GARwith\\nsparse representations performs similarly or better\\nthan state-of-the-art methods based on the dense\\nrepresentations of the original queries. GARcan\\nalso be easily combined with dense representa-\\ntions to produce even better results. Furthermore,\\nGARachieves state-of-the-art end-to-end perfor-\\nmance on extractive OpenQA and competitive per-\\nformance under the generative setup.\\n8 Future Extensions\\nPotential improvements . There is still much\\nspace to explore and improve for GARin future\\nwork. For query context generation, one can ex-\\nplore multi-task learning to further reduce computa-\\ntional cost and examine whether different contextscan mutually enhance each other when generated\\nby the same generator. One may also sample multi-\\nple contexts instead of greedy decoding to enrich a\\nquery. For retrieval, one can adopt more advanced\\nfusion techniques based on both the ranking and\\nscore of the passages. As the generator and re-\\ntriever are largely independent now, it is also inter-\\nesting to study how to jointly or iteratively optimize\\ngeneration and retrieval such that the generator is\\naware of the retriever and generates query contexts\\nmore beneﬁcial for the retrieval stage. Last but not\\nleast, it is very likely that better results can be ob-\\ntained by more extensive hyper-parameter tuning.\\nApplicability to other tasks . Beyond OpenQA,\\nGARalso has great potentials for other tasks that\\ninvolve text matching such as conversation utter-\\nance selection (Lowe et al., 2015; Dinan et al.,\\n2020) or information retrieval (Nguyen et al., 2016;\\nCraswell et al., 2020). The default generation tar-\\nget is always available for supervised tasks. For\\nexample, for conversation utterance selection one\\ncan use the reference utterance as the default target\\nand then match the concatenation of the conversa-\\ntion history and the generated utterance with the\\nprovided utterance candidates. For article search,\\nthe default target could be (part of) the ground-truth\\narticle itself. Other generation targets are more task-\\nspeciﬁc and can be designed as long as they can\\nbe fetched from the latent knowledge inside PLMs\\nand are helpful for further text retrieval (matching).\\nNote that by augmenting (expanding) the queries\\nwith heuristically discovered relevant contexts ex-\\ntracted from PLMs instead of reformulating them,\\nGARbypasses the need for external supervision to\\nform the original-reformulated query pairs.\\nAcknowledgments\\nWe thank Vladimir Karpukhin, Sewon Min, Gau-\\ntier Izacard, Wenda Qiu, Revanth Reddy, and Hao\\nCheng for helpful discussions. We thank the anony-\\nmous reviewers for valuable comments.\\nReferences\\nNasreen Abdul-Jaleel, James Allan, W Bruce Croft,\\nFernando Diaz, Leah Larkey, Xiaoyan Li, Mark D\\nSmucker, and Courtney Wade. 2004. Umass at trec\\n2004: Novelty and hard. Computer Science Depart-\\nment Faculty Publication Series , page 189.\\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\\nRichard Socher, and Caiming Xiong. 2019. Learn-\\ning to retrieve reasoning paths over wikipedia', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82eb12ad-7f0d-4c7e-b634-cb97ac66f72f', embedding=None, metadata={'page_label': '10', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ce9f8fb-1203-491e-9613-8cfa50afb2e3', embedding=None, metadata={'page_label': '11', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle\\nPineau. 2015. The ubuntu dialogue corpus: A large\\ndataset for research in unstructured multi-turn dia-\\nlogue systems. arXiv preprint arXiv:1506.08909 .\\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\\nMichael Collins. 2020. Sparse, dense, and at-\\ntentional representations for text retrieval. arXiv\\npreprint arXiv:2005.00181 .\\nYuanhua Lv and ChengXiang Zhai. 2010. Positional\\nrelevance model for pseudo-relevance feedback. In\\nProceedings of the 33rd international ACM SIGIR\\nconference on Research and development in infor-\\nmation retrieval , pages 579–586.\\nYuning Mao, Xiang Ren, Heng Ji, and Jiawei Han.\\n2020. Constrained abstractive summarization: Pre-\\nserving factual consistency with constrained genera-\\ntion. arXiv preprint arXiv:2010.12723 .\\nSewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi\\nChen, Eunsol Choi, Michael Collins, Kelvin Guu,\\nHannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-\\nmaki, et al. 2021. Neurips 2020 efﬁcientqa compe-\\ntition: Systems, analyses and lessons learned. arXiv\\npreprint arXiv:2101.00133 .\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\\nproach for weakly supervised question answering.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 2851–\\n2864, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\\ntrieval and reading for open domain question answer-\\ning. arXiv preprint arXiv:1911.03868 .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. Ambigqa: Answering\\nambiguous open-domain questions. arXiv preprint\\narXiv:2004.10645 .\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\\n2016. Ms marco: A human-generated machine read-\\ning comprehension dataset.\\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\\noriented query reformulation with reinforcement\\nlearning. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 574–583, Copenhagen, Denmark. Association\\nfor Computational Linguistics.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the pa-\\nrameters of a language model? arXiv preprint\\narXiv:2002.08910 .Joseph Rocchio. 1971. Relevance feedback in in-\\nformation retrieval. The Smart retrieval system-\\nexperiments in automatic document processing ,\\npages 313–323.\\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\\nand Raviteja Anantha. 2020. Question rewriting for\\nconversational question answering. arXiv preprint\\narXiv:2004.14652 .\\nXiao Wang, Craig Macdonald, and Iadh Ounis. 2020.\\nDeep reinforced query reformulation for informa-\\ntion retrieval. arXiv preprint arXiv:2007.07987 .\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\\nEnabling the use of lucene for information retrieval\\nresearch. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval , pages 1253–1256.\\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\\nPaul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020.\\nFew-shot generative conversational query rewriting.\\narXiv preprint arXiv:2006.05009 .\\nSalah Zaiem and Fatiha Sadat. 2019. Sequence to se-\\nquence learning for query expansion. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelli-\\ngence, Student Abstract Track , volume 33, pages\\n10075–10076.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35876d18-5d12-4d21-a97a-e231d3c3e167', embedding=None, metadata={'page_label': '12', 'file_name': 'Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_path': 'd:\\\\My_Project\\\\Project Langchain\\\\RAG LLM App\\\\data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'file_type': 'application/pdf', 'file_size': 1912776, 'creation_date': '2024-03-30', 'last_modified_date': '2024-03-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A More Analysis of Retrieval\\nPerformance\\nWe show the detailed results of top-k retrieval accu-\\nracy of the compared methods in Figs. 2 and 3.\\nGARperforms comparably or better than DPR\\nwhen k≥100on NQ and k≥5on Trivia.\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages2030405060708090Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 2: Top-k retrieval accuracy of sparse and\\ndense methods on the test set of NQ. GARimproves\\nBM25 and achieves comparable or better performance\\nthan DPR when k≥100.\\n1 5 10 20 50 100\\nk: # of retrieved passages5055606570758085Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 3: Top-k retrieval accuracy on the Trivia test\\nset.GARachieves better results than DPR when k≥5.\\nWe show in Table 9 the retrieval accuracy break-\\ndown using the question-answer overlap categories.\\nThe most signiﬁcant gap between BM25 and other\\nmethods is on the Question Overlap category,\\nwhich coincides with the fact that BM25 is un-\\nable to conduct question paraphrasing (semantic\\nmatching). GARhelps BM25 to bridge the gap by\\nproviding the query contexts and even outperform\\nDPR in this category. Moreover, GARconsistently\\nimproves over BM25 on other categories and GAR+\\noutperforms DPR as well.Method TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nBM25 78.8 81.2 85.1 70.6\\nDPR 86.1 93.2 89.5 76.8\\nGAR 85.3 94.1 87.9 73.7\\nGAR+88.9 96.3 91.7 79.8\\nTable 9: Top-100 retrieval accuracy by question-\\nanswer overlap categories on the NQ test set.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My_Project\\Project Langchain\\RAG LLM App\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 56/56 [00:00<00:00, 71.09it/s]\n",
      "Generating embeddings: 100%|██████████| 92/92 [00:04<00:00, 21.80it/s]\n"
     ]
    }
   ],
   "source": [
    "index=VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x219d4239420>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever=VectorIndexRetriever(index=index,similarity_top_k=4)\n",
    "postprocessor=SimilarityPostprocessor(similarity_cutoff=0.70)\n",
    "\n",
    "query_engine=RetrieverQueryEngine(retriever=retriever,node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=query_engine.query(\"What is Retrieval Augmented Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Retrieval-Augmented Generation (RAG) is a method that\n",
      "involves incorporating external knowledge via information retrieval to\n",
      "enhance the performance of large language models (LLMs). It is\n",
      "considered a promising approach to address challenges faced by LLMs\n",
      "such as factual hallucination, knowledge outdatedness, and lack of\n",
      "domain-specific expertise. RAG typically works by retrieving relevant\n",
      "information from external knowledge sources based on the input\n",
      "provided, and then generating a complete answer or response using this\n",
      "retrieved information.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: b17435e5-3b1f-41d0-83d5-9cf7a1cd9ebd\n",
      "Similarity: 0.8584384851480056\n",
      "Text: Benchmarking Large Language Models in Retrieval-Augmented\n",
      "Generation Jiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2\n",
      "1Chinese Information Processing Laboratory, Institute of Software,\n",
      "Chinese Academy of Sciences, Beijing, China 2State Key Laboratory of\n",
      "Computer Science, Institute of Software, Chinese Academy of Sciences,\n",
      "Beijing, Chin...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 1aedf8d9-201a-491c-a7fa-19e9933bc9d1\n",
      "Similarity: 0.8525907974251469\n",
      "Text: However, these methods re- quire either task-speciﬁc data (\n",
      "e.g., conversational contexts, ill-formed queries) or external\n",
      "resources such as paraphrase data (Zaiem and Sadat, 2019; Wang et al.,\n",
      "2020) that cannot or do not trans- fer well to OpenQA. Also, some rely\n",
      "on time- consuming training process like reinforcement learning (RL)\n",
      "(Nogueira and...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: db74168d-3fbc-404a-ac7a-c8046d2eaba8\n",
      "Similarity: 0.8522238160826183\n",
      "Text: Active Retrieval Augmented Generation Zhengbao Jiang1∗Frank F.\n",
      "Xu1∗Luyu Gao1∗Zhiqing Sun1∗Qian Liu2 Jane Dwivedi-Yu3Yiming Yang1Jamie\n",
      "Callan1Graham Neubig1 1Language Technologies Institute, Carnegie\n",
      "Mellon University 2Sea AI Lab3FAIR, Meta\n",
      "{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite\n",
      "the remarkable ability of large lan-...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 7a9e26b8-7b6c-4b25-a9f6-4857ea0c5adf\n",
      "Similarity: 0.8446729547709527\n",
      "Text: sidering the impressive performance achieved by GPT-3.5 (Ouyang\n",
      "et al., 2022) on a variety of tasks, we examine the effectiveness of\n",
      "our meth- ods on text-davinci-003 . We evaluate FLARE on 4 diverse\n",
      "tasks/datasets involving generating long outputs, including multihop\n",
      "QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form\n",
      "QA (ASQA...\n",
      "Retrieval-Augmented Generation (RAG) is a method that involves incorporating external knowledge via information retrieval to enhance the performance of large language models (LLMs). It is considered a promising approach to address challenges faced by LLMs such as factual hallucination, knowledge outdatedness, and lack of domain-specific expertise. RAG typically works by retrieving relevant information from external knowledge sources based on the input provided, and then generating a complete answer or response using this retrieved information.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(response,show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is an approach that incorporates external knowledge through information retrieval to enhance the performance of large language models (LLMs) by mitigating issues like hallucination.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Retrieval Augmented Generation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
